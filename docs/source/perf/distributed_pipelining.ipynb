{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Parallelism\n",
    "\n",
    "## Simple Pipeline\n",
    "\n",
    "No local SPMD or FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'CPU:0'},\n",
       " {'name': 'CPU:1'},\n",
       " {'name': 'CPU:2'},\n",
       " {'name': 'CPU:3'},\n",
       " {'name': 'CPU:4'},\n",
       " {'name': 'CPU:5'},\n",
       " {'name': 'CPU:6'},\n",
       " {'name': 'CPU:7'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WORLD_SIZE\"] = '8'\n",
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"CPU_NUM_DEVICES\"] = os.environ[\"WORLD_SIZE\"]\n",
    "os.environ[\"PJRT_DEVICE\"] = 'CPU'\n",
    "\n",
    "import torch_xla\n",
    "torch_xla.runtime.global_runtime_device_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleLinear(nn.Module):\n",
    "  NUM_CLASSES = 3\n",
    "\n",
    "  def __init__(self, input_dim):\n",
    "      super().__init__()\n",
    "      # Instead of Sequential, define layers separately for easier split points\n",
    "      self.layer0 = nn.Linear(input_dim, input_dim // 2)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.layer1 = nn.Linear(input_dim // 2, 3)\n",
    "      self.layer2 = nn.Linear(3, self.NUM_CLASSES)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.layer0(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.layer1(x)\n",
    "      x = self.layer2(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Pipeline - No FSDP or TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "\n",
    "class TrainingOptions():\n",
    "    def __init__(self):\n",
    "      self.batch_size = 128\n",
    "      self.num_epochs = 1\n",
    "      self.lr = 0.1\n",
    "      self.log_steps = 8\n",
    "      self.input_dim = 16834\n",
    "      self.train_dataset_len = 1024 * 8\n",
    "      self.pipeline_chunks = 2\n",
    "\n",
    "opts = TrainingOptions()\n",
    "device = 'cpu'\n",
    "\n",
    "model = SimpleLinear(opts.input_dim).to(device)\n",
    "\n",
    "# Define split points for pipeline parallelism\n",
    "split_spec = {\n",
    "  \"layer0\": SplitPoint.END,\n",
    "  \"layer1\": SplitPoint.END,\n",
    "}\n",
    "\n",
    "# Create a sample input for the pipeline\n",
    "chunks = opts.pipeline_chunks\n",
    "batch_size = opts.batch_size\n",
    "example_input = torch.randn(batch_size, opts.input_dim, device=device)\n",
    "\n",
    "# Make sure that program is full-graph capturable:\n",
    "# torch.export.export(model, (example_input,))\n",
    "\n",
    "# Create the pipeline and respective stage for the rank.\n",
    "pipe = pipeline(model, mb_args=(example_input,), split_spec=split_spec)\n",
    "# stage = ...\n",
    "#schedule = ScheduleGPipe(stage, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a Pipeline\n",
    "\n",
    "Note that this pipeline contains all the individual stages of interest.\n",
    "Given that this was formed from `torch.export` it seems plausible that we can\n",
    "get each sub-module asa a separately traced StableHLO program to be stitched\n",
    "together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E Pipeline - Contains all submodules and a coordinating function.\n",
      "GraphModule(\n",
      "  (submod_0): GraphModule(\n",
      "    (layer0): InterpreterModule()\n",
      "  )\n",
      "  (submod_1): GraphModule(\n",
      "    (relu): InterpreterModule()\n",
      "    (layer1): InterpreterModule()\n",
      "  )\n",
      "  (submod_2): GraphModule(\n",
      "    (layer2): InterpreterModule()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    submod_0 = self.submod_0(x);  x = None\n",
      "    submod_1 = self.submod_1(submod_0);  submod_0 = None\n",
      "    submod_2 = self.submod_2(submod_1);  submod_1 = None\n",
      "    return (submod_2,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "Individual stages are fx.GraphModules\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        x: \"f32[128, 16834]\"; \n",
      "    \n",
      "        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\n",
      "        # No stacktrace found for following nodes\n",
      "        layer0: \"f32[128, 8417]\" = self.layer0(x);  x = None\n",
      "        return layer0\n",
      "        \n",
      "    class layer0(torch.nn.Module):\n",
      "        def forward(self, x: \"f32[128, 16834]\"):\n",
      "            # No stacktrace found for following nodes\n",
      "            weight: \"f32[8417, 16834]\" = self.weight\n",
      "            bias: \"f32[8417]\" = self.bias\n",
      "            \n",
      "             # File: /usr/local/google/home/gleasonk/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/pipelining/_IR.py:1168 in _split_after_forward, code: return self._orig_forward(*args, **kwargs)\n",
      "            linear_default: \"f32[128, 8417]\" = torch.ops.aten.linear.default(x, weight, bias);  x = weight = bias = None\n",
      "            return linear_default\n",
      "            \n",
      "Stage 0:\n",
      " class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        x: \"f32[128, 16834]\"; \n",
      "    \n",
      "        x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\n",
      "        # No stacktrace found for following nodes\n",
      "        layer0: \"f32[128, 8417]\" = self.layer0(x);  x = None\n",
      "        return layer0\n",
      "        \n",
      "    class layer0(torch.nn.Module):\n",
      "        def forward(self, x: \"f32[128, 16834]\"):\n",
      "            # No stacktrace found for following nodes\n",
      "            weight: \"f32[8417, 16834]\" = self.weight\n",
      "            bias: \"f32[8417]\" = self.bias\n",
      "            \n",
      "             # File: /usr/local/google/home/gleasonk/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/pipelining/_IR.py:1168 in _split_after_forward, code: return self._orig_forward(*args, **kwargs)\n",
      "            linear_default: \"f32[128, 8417]\" = torch.ops.aten.linear.default(x, weight, bias);  x = weight = bias = None\n",
      "            return linear_default\n",
      "            \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, linear: \"f32[128, 8417]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        relu: \"f32[128, 8417]\" = self.relu(linear);  linear = None\n",
      "        layer1: \"f32[128, 3]\" = self.layer1(relu);  relu = None\n",
      "        return layer1\n",
      "        \n",
      "    class relu(torch.nn.Module):\n",
      "        def forward(self, linear: \"f32[128, 8417]\"):\n",
      "             # File: /tmp/ipykernel_475282/1157330327.py:21 in forward, code: x = self.relu(x)\n",
      "            relu_default: \"f32[128, 8417]\" = torch.ops.aten.relu.default(linear);  linear = None\n",
      "            return relu_default\n",
      "            \n",
      "    class layer1(torch.nn.Module):\n",
      "        def forward(self, relu_default: \"f32[128, 8417]\"):\n",
      "            # No stacktrace found for following nodes\n",
      "            weight: \"f32[3, 8417]\" = self.weight\n",
      "            bias: \"f32[3]\" = self.bias\n",
      "            \n",
      "             # File: /usr/local/google/home/gleasonk/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/pipelining/_IR.py:1168 in _split_after_forward, code: return self._orig_forward(*args, **kwargs)\n",
      "            linear_default: \"f32[128, 3]\" = torch.ops.aten.linear.default(relu_default, weight, bias);  relu_default = weight = bias = None\n",
      "            return linear_default\n",
      "            \n",
      "Stage 1:\n",
      " class GraphModule(torch.nn.Module):\n",
      "    def forward(self, linear: \"f32[128, 8417]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        relu: \"f32[128, 8417]\" = self.relu(linear);  linear = None\n",
      "        layer1: \"f32[128, 3]\" = self.layer1(relu);  relu = None\n",
      "        return layer1\n",
      "        \n",
      "    class relu(torch.nn.Module):\n",
      "        def forward(self, linear: \"f32[128, 8417]\"):\n",
      "             # File: /tmp/ipykernel_475282/1157330327.py:21 in forward, code: x = self.relu(x)\n",
      "            relu_default: \"f32[128, 8417]\" = torch.ops.aten.relu.default(linear);  linear = None\n",
      "            return relu_default\n",
      "            \n",
      "    class layer1(torch.nn.Module):\n",
      "        def forward(self, relu_default: \"f32[128, 8417]\"):\n",
      "            # No stacktrace found for following nodes\n",
      "            weight: \"f32[3, 8417]\" = self.weight\n",
      "            bias: \"f32[3]\" = self.bias\n",
      "            \n",
      "             # File: /usr/local/google/home/gleasonk/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/pipelining/_IR.py:1168 in _split_after_forward, code: return self._orig_forward(*args, **kwargs)\n",
      "            linear_default: \"f32[128, 3]\" = torch.ops.aten.linear.default(relu_default, weight, bias);  relu_default = weight = bias = None\n",
      "            return linear_default\n",
      "            \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, linear_1: \"f32[128, 3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        layer2: \"f32[128, 3]\" = self.layer2(linear_1);  linear_1 = None\n",
      "        return layer2\n",
      "        \n",
      "    class layer2(torch.nn.Module):\n",
      "        def forward(self, linear_1: \"f32[128, 3]\"):\n",
      "            # No stacktrace found for following nodes\n",
      "            weight: \"f32[3, 3]\" = self.weight\n",
      "            bias: \"f32[3]\" = self.bias\n",
      "            \n",
      "             # File: /tmp/ipykernel_475282/1157330327.py:23 in forward, code: x = self.layer2(x)\n",
      "            linear_default: \"f32[128, 3]\" = torch.ops.aten.linear.default(linear_1, weight, bias);  linear_1 = weight = bias = None\n",
      "            return linear_default\n",
      "            \n",
      "Stage 2:\n",
      " class GraphModule(torch.nn.Module):\n",
      "    def forward(self, linear_1: \"f32[128, 3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        layer2: \"f32[128, 3]\" = self.layer2(linear_1);  linear_1 = None\n",
      "        return layer2\n",
      "        \n",
      "    class layer2(torch.nn.Module):\n",
      "        def forward(self, linear_1: \"f32[128, 3]\"):\n",
      "            # No stacktrace found for following nodes\n",
      "            weight: \"f32[3, 3]\" = self.weight\n",
      "            bias: \"f32[3]\" = self.bias\n",
      "            \n",
      "             # File: /tmp/ipykernel_475282/1157330327.py:23 in forward, code: x = self.layer2(x)\n",
      "            linear_default: \"f32[128, 3]\" = torch.ops.aten.linear.default(linear_1, weight, bias);  linear_1 = weight = bias = None\n",
      "            return linear_default\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "print(\"E2E Pipeline - Contains all submodules and a coordinating function.\")\n",
    "print(pipe)\n",
    "\n",
    "print(\"Individual stages are fx.GraphModules\")\n",
    "for idx in range(pipe.num_stages):\n",
    "  print(f\"Stage {idx}:\\n\", pipe.get_stage_module(idx).print_readable())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WIP] Training Loop with Pipelining\n",
    "\n",
    "Note that pipelining relies on `torch.export` and requires a fully functional\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.runtime as xr\n",
    "import torch_xla.distributed.xla_backend\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.pipelining import ScheduleGPipe, SplitPoint, pipeline, PipelineStage\n",
    "from torch.distributed.tensor import init_device_mesh, Shard, distribute_tensor\n",
    "import torch_xla.distributed.spmd as xs\n",
    "\n",
    "def train(training_options):\n",
    "  print(training_options)\n",
    "  # Torchrun is needed for Pipeline Parallelism by default. Generally, we\n",
    "  # don't need it for SPMD, and we could rely on `process_index` and\n",
    "  # `addressable_runtime_device_count` from PjRT runtime. However, it would\n",
    "  # be needed if we have multiple SPMD worlds within the same physical machine.\n",
    "  # Hence, we retain the requirement, and can relax it later on.\n",
    "  rank = xr.process_index() # int(os.environ[\"RANK\"]) \n",
    "  print(\"Addressable:\", xr.addressable_runtime_device_count())\n",
    "  chunks = training_options.pipeline_chunks\n",
    " \n",
    "  # Use XLA device\n",
    "  device = xm.xla_device()\n",
    "\n",
    "  print(f\"Rank {rank} using device {device}\")\n",
    "  num_devices = xr.global_runtime_device_count()\n",
    "\n",
    "  # -----\n",
    "  # (Preferred) Leverage the DTensor/DeviceMesh variants for a more seamless\n",
    "  # user interface with submeshes.\n",
    "  # -----\n",
    "  # global_mesh = init_device_mesh(\"xla\", (chunks, num_devices, 1),\n",
    "  #                                mesh_dim_names=(\"pp\", \"data\", \"model\"))\n",
    "  # local_mesh = global_mesh[\"data\", \"model\"]\n",
    "\n",
    "  # -----\n",
    "  # Alternatively:\n",
    "  # -----\n",
    "  num_local_devices = xr.addressable_runtime_device_count() // chunks\n",
    " \n",
    "  # Global submesh\n",
    "  global_mesh_shape = (chunks, num_local_devices, 1)\n",
    "  global_mesh = xs.Mesh(np.arange(num_devices), global_mesh_shape, (\"pp\", \"data\", \"model\"))\n",
    " \n",
    "  # Local submesh\n",
    "  device_id_start = rank * num_local_devices\n",
    "  local_device_ids = np.arange(device_id_start, device_id_start + num_local_devices)\n",
    "  local_mesh_shape = global_mesh_shape[1:]\n",
    "  local_mesh = xs.Mesh(local_device_ids, local_mesh_shape, (\"data\", \"model\"))\n",
    "  # -----\n",
    "\n",
    "  # Initialize process group\n",
    "  dist.init_process_group(\n",
    "      backend=\"xla\",\n",
    "      init_method=\"xla://\",\n",
    "      rank=rank,\n",
    "      world_size=world_size\n",
    "  )\n",
    "\n",
    "  torch.manual_seed(42)\n",
    "  model = SimpleLinear(training_options.input_dim).to(device)\n",
    "\n",
    "  # Shard the model weights as needed:\n",
    "  # parallelize_model(model, local_mesh)\n",
    "\n",
    "  # Define split points for pipeline parallelism\n",
    "  split_spec = {\n",
    "    \"layer0\": SplitPoint.END,\n",
    "    \"layer1\": SplitPoint.END,\n",
    "  }\n",
    "\n",
    "  # Create a sample input for the pipeline\n",
    "  batch_size = training_options.batch_size\n",
    "  example_input = torch.randn(batch_size, training_options.input_dim, device=device)\n",
    "\n",
    "  # Create the pipeline and respective stage for the rank.\n",
    "  pipe = pipeline(model, chunks,  mb_args=(example_input,), split_spec=split_spec)\n",
    "  stage = PipelineStage(pipe, rank, device)\n",
    "  schedule = ScheduleGPipe(stage, chunks)\n",
    "\n",
    "  # Training loop\n",
    "  losses = []\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=training_options.lr)\n",
    "\n",
    "  for epoch in range(training_options.num_epochs):\n",
    "    for step, (data, target) in enumerate(data_generator()):\n",
    "      if rank == 0:\n",
    "        xs.mark_sharding(data, local_mesh, ('data', 'model'))\n",
    "        # or distribute_tensor(data, local_mesh, [Shard(0), Shard(1)])\n",
    "        schedule.step(data)\n",
    "        optimizer.zero_grad()\n",
    "      else:\n",
    "        output = schedule.step()\n",
    "        # Only the last rank computes loss and does backward\n",
    "        if rank == world_size - 1:\n",
    "          loss = loss_fn(output, target)\n",
    "          losses.append(loss.clone().detach())\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          if step % training_options.log_steps == 0:\n",
    "            print(f\"Epoch {epoch} step {step} loss {loss}\")\n",
    "      xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training loop...\n",
      "<__main__.TrainingOptions object at 0x7fcaec0f8a50>\n",
      "Addressable: 8\n",
      "Rank 0 using device xla:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/torch_xla/runtime.py:236: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
      "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of device IDs (4) must match the global number of devices (8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m       \u001b[38;5;28mself\u001b[39m.pipeline_chunks = \u001b[32m2\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mStart training loop...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainingOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m dist.destroy_process_group()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(training_options)\u001b[39m\n\u001b[32m     56\u001b[39m local_device_ids = np.arange(device_id_start, device_id_start + num_local_devices)\n\u001b[32m     57\u001b[39m local_mesh_shape = global_mesh_shape[\u001b[32m1\u001b[39m:]\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m local_mesh = \u001b[43mxs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_device_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_mesh_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# -----\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Initialize process group\u001b[39;00m\n\u001b[32m     62\u001b[39m dist.init_process_group(\n\u001b[32m     63\u001b[39m     backend=\u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     64\u001b[39m     init_method=\u001b[33m\"\u001b[39m\u001b[33mxla://\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m     rank=rank,\n\u001b[32m     66\u001b[39m     world_size=world_size\n\u001b[32m     67\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:81\u001b[39m, in \u001b[36mMesh.__init__\u001b[39m\u001b[34m(self, device_ids, mesh_shape, axis_names)\u001b[39m\n\u001b[32m     79\u001b[39m num_devices = xr.global_runtime_device_count()\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m num_devices > \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThis requires XLA supported device(s).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m num_devices == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m     82\u001b[39m     device_ids\n\u001b[32m     83\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of device IDs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(device_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must match the global number of devices (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     86\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mesh_shape) == \u001b[38;5;28mlen\u001b[39m(axis_names), \\\n\u001b[32m     87\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of axis names (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(axis_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must match mesh dimensions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mesh_shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Number of device IDs (4) must match the global number of devices (8)"
     ]
    }
   ],
   "source": [
    "xr.use_spmd()\n",
    "\n",
    "class TrainingOptions():\n",
    "    def __init__(self):\n",
    "      self.batch_size = 128\n",
    "      self.num_epochs = 1\n",
    "      self.lr = 0.1\n",
    "      self.log_steps = 8\n",
    "      self.input_dim = 16834\n",
    "      self.train_dataset_len = 1024 * 8\n",
    "      self.pipeline_chunks = 2\n",
    "\n",
    "print('Start training loop...')\n",
    "train(TrainingOptions())\n",
    "dist.destroy_process_group()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptxla.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
