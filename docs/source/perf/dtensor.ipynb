{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch/XLA DTensor Integration\n",
    "\n",
    "This notebook focuses on the application of DTensor with PyTorch/XLA.\n",
    "\n",
    "See internal implementation details at\n",
    "[[RFC] XLA Lazy Backend Support In DistributedTensor API #92909][dtensor-rfc]\n",
    "\n",
    "[dtensor-rfc]: https://github.com/pytorch/pytorch/issues/92909\n",
    "\n",
    "This can be run in command line using:\n",
    "\n",
    "```\n",
    "# Install jupyter\n",
    "$ apt install jupyter\n",
    "\n",
    "# Create Jupyter ipy kernel for current development venv:\n",
    "$ pip install ipykernel\n",
    "$ python -m ipykernel install --user --name=ptxla.venv\n",
    "\n",
    "# \n",
    "$ jupyter execute --kernel_name=ptxla.venv /usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/docs/source/perf/dtensor.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parallel environment\n",
    "\n",
    "We'll fake an 8 CPU setup. Note this must be done before the XLA PJRT plugin is\n",
    "initialized by PyTorch/XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'CPU:0'}, {'name': 'CPU:1'}, {'name': 'CPU:2'}, {'name': 'CPU:3'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WORLD_SIZE\"] = '4'\n",
    "os.environ[\"CPU_NUM_DEVICES\"] = os.environ[\"WORLD_SIZE\"]\n",
    "os.environ[\"PJRT_DEVICE\"] = 'CPU'\n",
    "\n",
    "import torch_xla\n",
    "torch_xla.runtime.global_runtime_device_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to DTensor\n",
    "\n",
    "The following sections are intended to mirror the PyTorch natice DTensor\n",
    "tutorial:\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/torch_xla/runtime.py:236: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
      "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLAShardedTensor(tensor([[-1.7080,  0.4637, -1.4058,  ..., -1.6900, -1.1051,  1.7210],\n",
      "        [-0.3269, -1.5310,  0.4159,  ...,  0.1561,  1.0350,  0.2642],\n",
      "        [ 0.8611,  0.5285,  1.2459,  ...,  0.2676, -1.2947,  1.0346],\n",
      "        ...,\n",
      "        [ 0.1595, -0.1214,  0.4675,  ..., -0.6620,  0.4335, -0.5811],\n",
      "        [-0.1768,  0.3318,  1.4751,  ..., -0.3326,  0.0671,  1.1855],\n",
      "        [-0.3218, -0.6718, -0.2988,  ...,  0.2206, -0.4058,  1.0825]],\n",
      "       device='xla:0'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/gleasonk/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:351: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#introduction\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.distributed.tensor import init_device_mesh, Shard, distribute_tensor\n",
    "\n",
    "# Create a mesh topology with the available devices:\n",
    "# 1. We can directly create the mesh using elastic launcher, (recommended)\n",
    "# 2. If using mp.spawn, one need to initialize the world process_group first and set device\n",
    "#   i.e. torch.distributed.init_process_group(backend=\"nccl\", world_size=world_size)\n",
    "\n",
    "mesh = init_device_mesh(\"xla\", (int(os.environ[\"WORLD_SIZE\"]),))\n",
    "big_tensor = torch.randn(100000, 88)\n",
    "# Shard this tensor over the mesh by sharding `big_tensor`'s 0th dimension over the 0th dimension of `mesh`.\n",
    "my_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)])\n",
    "\n",
    "print(my_dtensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DTensor Examples\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#basic-dtensor-api-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLAShardedTensor(tensor([[ 2.7933, -1.2275, -0.2549,  ...,  0.9429, -0.2904, -1.2301],\n",
      "        [ 0.8740, -1.5658,  2.2909,  ..., -0.3352,  0.3340,  1.7716],\n",
      "        [ 1.9126, -2.0719, -2.1520,  ...,  1.7489,  0.9192, -0.9709],\n",
      "        ...,\n",
      "        [ 1.4185, -0.0687,  0.3937,  ...,  0.2070, -0.9086,  1.4251],\n",
      "        [ 1.6532, -0.4115,  0.9860,  ..., -0.6602,  1.1456,  0.7818],\n",
      "        [-0.3824,  0.0094, -0.2849,  ..., -2.6232,  0.1736, -0.6748]],\n",
      "       device='xla:0'))\n",
      "XLAShardedTensor(tensor([[ 2.7933, -1.2275, -0.2549,  ...,  0.9429, -0.2904, -1.2301],\n",
      "        [ 0.8740, -1.5658,  2.2909,  ..., -0.3352,  0.3340,  1.7716],\n",
      "        [ 1.9126, -2.0719, -2.1520,  ...,  1.7489,  0.9192, -0.9709],\n",
      "        ...,\n",
      "        [ 1.4185, -0.0687,  0.3937,  ...,  0.2070, -0.9086,  1.4251],\n",
      "        [ 1.6532, -0.4115,  0.9860,  ..., -0.6602,  1.1456,  0.7818],\n",
      "        [-0.3824,  0.0094, -0.2849,  ..., -2.6232,  0.1736, -0.6748]],\n",
      "       device='xla:0'))\n",
      "XLAShardedTensor(tensor([[ 2.7933, -1.2275, -0.2549,  ...,  0.9429, -0.2904, -1.2301],\n",
      "        [ 0.8740, -1.5658,  2.2909,  ..., -0.3352,  0.3340,  1.7716],\n",
      "        [ 1.9126, -2.0719, -2.1520,  ...,  1.7489,  0.9192, -0.9709],\n",
      "        ...,\n",
      "        [ 1.4185, -0.0687,  0.3937,  ...,  0.2070, -0.9086,  1.4251],\n",
      "        [ 1.6532, -0.4115,  0.9860,  ..., -0.6602,  1.1456,  0.7818],\n",
      "        [-0.3824,  0.0094, -0.2849,  ..., -2.6232,  0.1736, -0.6748]],\n",
      "       device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributed.tensor import DTensor, Shard, Replicate, distribute_tensor, distribute_module, init_device_mesh\n",
    "\n",
    "# construct a device mesh with available devices (multi-host or single host)\n",
    "device_mesh = init_device_mesh(\"xla\", (4,))\n",
    "# if we want to do row-wise sharding\n",
    "rowwise_placement=[Shard(0)]\n",
    "# if we want to do col-wise sharding\n",
    "colwise_placement=[Shard(1)]\n",
    "\n",
    "big_tensor = torch.randn(888, 12)\n",
    "# distributed tensor returned will be sharded across the dimension specified in placements\n",
    "rowwise_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=rowwise_placement)\n",
    "\n",
    "# if we want to do replication across a certain device list\n",
    "replica_placement = [Replicate()]\n",
    "# distributed tensor will be replicated to all four GPUs.\n",
    "replica_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=replica_placement)\n",
    "\n",
    "# if we want to distributed a tensor with both replication and sharding\n",
    "device_mesh = init_device_mesh(\"xla\", (2, 2))\n",
    "# replicate across the first dimension of device mesh, then sharding on the second dimension of device mesh\n",
    "spec=[Replicate(), Shard(0)]\n",
    "partial_replica = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=spec)\n",
    "\n",
    "\n",
    "print(rowwise_tensor)\n",
    "print(replica_tensor)\n",
    "print(partial_replica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UX ISSUE: `DTensor.from_local` fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeviceMesh' object has no attribute '_coordinate_on_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\u001b[39;00m\n\u001b[32m      2\u001b[39m local_tensor = torch.randn((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m rowwise_tensor = \u001b[43mDTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowwise_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# reshard the current row-wise tensor to a colwise tensor or replicate tensor\u001b[39;00m\n\u001b[32m      6\u001b[39m colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:432\u001b[39m, in \u001b[36mDTensor.from_local\u001b[39m\u001b[34m(local_tensor, device_mesh, placements, run_check, shape, stride)\u001b[39m\n\u001b[32m    427\u001b[39m                 placements[idx] = Shard(placement.dim + local_tensor.ndim)\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# `from_local` is differentiable, and the gradient of the dist tensor this function\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# created should flow back the gradients to the local_tensor, so we call an autograd\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# function to construct the dist tensor instead.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FromTorchTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pyre-ignore[16]: autograd func\u001b[39;49;00m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplacements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:148\u001b[39m, in \u001b[36m_FromTorchTensor.forward\u001b[39m\u001b[34m(ctx, input, device_mesh, placements, run_check, shape, stride)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    144\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, stride:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstride\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass both shape and stride at the same time.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdevice_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# if the global rank is not participating in the device mesh, we\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# simply set the local tensor to an empty tensor\u001b[39;00m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.new_empty(\u001b[32m0\u001b[39m, requires_grad=\u001b[38;5;28minput\u001b[39m.requires_grad)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m run_check:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# TODO: support uneven sharding when global shape/stride not passed, by\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# building the global TensorMeta during check_tensor_meta\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/device_mesh.py:957\u001b[39m, in \u001b[36mDeviceMesh.get_coordinate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_coordinate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    953\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[33;03m    Return the relative indices of this rank relative to all\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[33;03m    dimensions of the mesh. If this rank is not part of the mesh, return None.\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coordinate_on_dim \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_coordinate_on_dim\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'DeviceMesh' object has no attribute '_coordinate_on_dim'"
     ]
    }
   ],
   "source": [
    "# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
    "local_tensor = torch.randn((8, 8), requires_grad=True)\n",
    "rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n",
    "\n",
    "# reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
    "colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
    "replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted workaround\n",
    "\n",
    "Sort of... New error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeviceMesh' object has no attribute '_dim_group_infos'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# reshard the current row-wise tensor to a colwise tensor or replicate tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m colwise_tensor = \u001b[43mrowwise_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mredistribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolwise_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:556\u001b[39m, in \u001b[36mDTensor.redistribute\u001b[39m\u001b[34m(self, device_mesh, placements, async_op, forward_dtype, backward_dtype)\u001b[39m\n\u001b[32m    553\u001b[39m placements = \u001b[38;5;28mtuple\u001b[39m(placements)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# pyre-fixme[16]: `Redistribute` has no attribute `apply`.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRedistribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masync_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_dtype\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_redistribute.py:325\u001b[39m, in \u001b[36mRedistribute.forward\u001b[39m\u001b[34m(ctx, input, device_mesh, placements, async_op, forward_dtype, backward_dtype)\u001b[39m\n\u001b[32m    320\u001b[39m     target_spec = DTensorSpec(\n\u001b[32m    321\u001b[39m         device_mesh, placements, tensor_meta=current_spec.tensor_meta\n\u001b[32m    322\u001b[39m     )\n\u001b[32m    324\u001b[39m     local_tensor = \u001b[38;5;28minput\u001b[39m._local_tensor\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     output = \u001b[43mredistribute_local_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masync_op\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_op\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# use the same local tensor if placements are the same.\u001b[39;00m\n\u001b[32m    330\u001b[39m     output = local_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_redistribute.py:239\u001b[39m, in \u001b[36mredistribute_local_tensor\u001b[39m\u001b[34m(local_tensor, current_spec, target_spec, async_op, is_backward)\u001b[39m\n\u001b[32m    237\u001b[39m         shard_spec = cast(Shard, current)\n\u001b[32m    238\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m shard_spec.dim != target_placement.dim:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m             new_local_tensor = \u001b[43mshard_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_new_shard_dim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtransform_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtarget_placement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m target.is_partial():\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current.is_replicate():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/placement_types.py:323\u001b[39m, in \u001b[36mShard._to_new_shard_dim\u001b[39m\u001b[34m(self, local_tensor, mesh, mesh_dim, current_logical_shape, new_shard_dim)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_tensor.is_contiguous():\n\u001b[32m    321\u001b[39m     local_tensor = local_tensor.contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m new_tensor = \u001b[43mshard_dim_alltoall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_shard_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_dim\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m old_dim_padding:\n\u001b[32m    328\u001b[39m     old_dim_unpad_size = (\n\u001b[32m    329\u001b[39m         old_dim_full_chunk_size * num_chunks - current_logical_shape[\u001b[38;5;28mself\u001b[39m.dim]  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[32m    330\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_collective_utils.py:67\u001b[39m, in \u001b[36mshard_dim_alltoall\u001b[39m\u001b[34m(input, gather_dim, shard_dim, mesh, mesh_dim)\u001b[39m\n\u001b[32m     62\u001b[39m     out = torch.chunk(out, mesh.size(mesh_dim), dim=shard_dim)[\n\u001b[32m     63\u001b[39m         mesh.get_local_rank(mesh_dim)\n\u001b[32m     64\u001b[39m     ]\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out.contiguous()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m group_name = \u001b[43mfuncol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_resolve_group_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# TODO: enable async op for shard_dim_alltoall\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.ops._dtensor.shard_dim_alltoall(\n\u001b[32m     70\u001b[39m     \u001b[38;5;28minput\u001b[39m, gather_dim, shard_dim, group_name\n\u001b[32m     71\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/_functional_collectives.py:779\u001b[39m, in \u001b[36m_resolve_group_name\u001b[39m\u001b[34m(group, tag)\u001b[39m\n\u001b[32m    777\u001b[39m     dmesh = group[\u001b[32m0\u001b[39m]\n\u001b[32m    778\u001b[39m     dim = group[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdmesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dim_group_infos\u001b[49m[dim][\u001b[32m2\u001b[39m]\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid tuple for group must be (DeviceMesh, int)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DeviceMesh' object has no attribute '_dim_group_infos'"
     ]
    }
   ],
   "source": [
    "# See: https://github.com/pytorch/xla/issues/8528\n",
    "# Need to stub the method in the meantime.\n",
    "# Not sure what to stub it to though.\n",
    "_rank = 0\n",
    "device_mesh._coordinate_on_dim = [_rank]  \n",
    "\n",
    "# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
    "local_tensor = torch.randn((8, 8), requires_grad=True)\n",
    "rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n",
    "\n",
    "# reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
    "colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
    "replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DummyMLP Example from Torch Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyMLP(\n",
       "  (net1): Linear(in_features=5, out_features=1024, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (net2): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DummyMLP(torch.nn.Module):\n",
    "  def __init__(self, device):\n",
    "    super().__init__()\n",
    "    self.net1 = torch.nn.Linear(5, 1024, device=device)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.net2 = torch.nn.Linear(1024, 4, device=device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net2(F.relu(self.net1(x)))\n",
    "\n",
    "  def reset_parameters(self, *args, **kwargs):\n",
    "    with torch.no_grad():\n",
    "      self.net1.weight.fill_(0.5)\n",
    "      self.net2.weight.fill_(1)\n",
    "      self.net1.bias.fill_(1.5)\n",
    "      self.net2.bias.fill_(1.2)\n",
    "\n",
    "DummyMLP(\"xla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UX ISSUE: Can't distribute using `parallelize_module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m model = DummyMLP(\u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m device_mesh = init_device_mesh(device_type, (\u001b[38;5;28mint\u001b[39m(os.environ[\u001b[33m\"\u001b[39m\u001b[33mWORLD_SIZE\u001b[39m\u001b[33m\"\u001b[39m]),))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m( \u001b[43mdevice_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m _rank = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m device_mesh._coordinate_on_dim = [_rank]  \u001b[38;5;66;03m# workaround for https://github.com/pytorch/xla/issues/8528\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/device_mesh.py:908\u001b[39m, in \u001b[36mDeviceMesh.get_rank\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_rank\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    905\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[33;03m    Returns the current global rank.\u001b[39;00m\n\u001b[32m    907\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:2301\u001b[39m, in \u001b[36mget_rank\u001b[39m\u001b[34m(group)\u001b[39m\n\u001b[32m   2298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[32m   2299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m default_pg = \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember.WORLD:\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg.rank()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1302\u001b[39m, in \u001b[36m_get_default_group\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefault process group has not been initialized, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease make sure to call init_process_group.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1305\u001b[39m     )\n\u001b[32m   1306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember.WORLD)\n",
      "\u001b[31mValueError\u001b[39m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "from torch.distributed.tensor.parallel import (\n",
    "    ColwiseParallel,\n",
    "    parallelize_module,\n",
    "    RowwiseParallel,\n",
    ")\n",
    "\n",
    "device_type = \"xla\" # was \"meta\" in test\n",
    "\n",
    "model = DummyMLP(\"xla\")\n",
    "device_mesh = init_device_mesh(device_type, (int(os.environ[\"WORLD_SIZE\"]),))\n",
    "\n",
    "# UX ISSUE: XLA and PyTorch handle \"RANK\" differently.\n",
    "# See: https://github.com/pytorch/xla/issues/8528\n",
    "# Need to stub the method in the meantime.\n",
    "# Not sure what to stub it to though.\n",
    "_rank = 0\n",
    "device_mesh._coordinate_on_dim = [_rank]  \n",
    "\n",
    "# UX ISSUE: We can't use upstream parallelization plans\n",
    "parallelize_plan = {\n",
    "    \"net1\": ColwiseParallel(),\n",
    "    \"net2\": RowwiseParallel(),\n",
    "}\n",
    "model_tp = parallelize_module(model, device_mesh, parallelize_plan)\n",
    "model_tp.to_empty(device=device_type)\n",
    "model_tp.reset_parameters()\n",
    "optim = torch.optim.SGD(model_tp.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "inp = torch.randn(20, 5, device=device_type)\n",
    "output = model_tp(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/torch_xla/runtime.py:236: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
      "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "!at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at \"/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp\":838, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(sharded_module)\n\u001b[32m     29\u001b[39m x = torch.rand((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43msharded_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mMyModule.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.fc2(\u001b[38;5;28minput\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharded_tensor.py:174\u001b[39m, in \u001b[36mXLAShardedTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__torch_function__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, func, types, args=(), kwargs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/_tensor.py:1668\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1668\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1670\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharded_tensor.py:169\u001b[39m, in \u001b[36mXLAShardedTensor.__torch_dispatch__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# no_dispatch is only needed if you use enable_python_mode.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# It prevents infinite recursion.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m no_dispatch():\n\u001b[32m    167\u001b[39m   \u001b[38;5;66;03m# re-dispatch to C++\u001b[39;00m\n\u001b[32m    168\u001b[39m   rs = tree_map(wrap,\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m                 \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/_ops.py:806\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: !at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at \"/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp\":838, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributed.tensor import Shard, distribute_tensor, distribute_module, init_device_mesh\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.relu(self.fc1(input) + self.fc2(input))\n",
    "\n",
    "mesh = init_device_mesh(\"xla\", (4,))\n",
    "\n",
    "def shard_params(mod_name, mod, mesh):\n",
    "    col_linear_placement = [Shard(0)]\n",
    "    # shard fc1 and fc2\n",
    "    if isinstance(mod, nn.Linear):\n",
    "        for name, param in mod.named_parameters():\n",
    "            dist_param = nn.Parameter(\n",
    "                distribute_tensor(param, mesh, col_linear_placement)\n",
    "            )\n",
    "            mod.register_parameter(name, dist_param)\n",
    "\n",
    "sharded_module = distribute_module(MyModule(), mesh, partition_fn=shard_params)\n",
    "print(sharded_module)\n",
    "\n",
    "x = torch.rand((8, 8))\n",
    "sharded_module(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptxla.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
