{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch/XLA DTensor Integration\n",
    "\n",
    "This notebook focuses on the application of DTensor with PyTorch/XLA.\n",
    "\n",
    "See internal implementation details at\n",
    "[[RFC] XLA Lazy Backend Support In DistributedTensor API #92909][dtensor-rfc]\n",
    "\n",
    "[dtensor-rfc]: https://github.com/pytorch/pytorch/issues/92909\n",
    "\n",
    "This can be run in command line using:\n",
    "\n",
    "```\n",
    "# Install jupyter\n",
    "$ apt install jupyter\n",
    "\n",
    "# Create Jupyter ipy kernel for current development venv:\n",
    "$ pip install ipykernel\n",
    "$ python -m ipykernel install --user --name=ptxla.venv\n",
    "\n",
    "# \n",
    "$ jupyter execute --kernel_name=ptxla.venv /usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/docs/source/perf/dtensor.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parallel environment\n",
    "\n",
    "We'll fake an 8 CPU setup. Note this must be done before the XLA PJRT plugin is\n",
    "initialized by PyTorch/XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'CPU:0'}, {'name': 'CPU:1'}, {'name': 'CPU:2'}, {'name': 'CPU:3'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WORLD_SIZE\"] = '4'\n",
    "os.environ[\"RANK\"] = '1'\n",
    "os.environ[\"CPU_NUM_DEVICES\"] = os.environ[\"WORLD_SIZE\"]\n",
    "os.environ[\"PJRT_DEVICE\"] = 'CPU'\n",
    "\n",
    "import torch_xla\n",
    "torch_xla.runtime.global_runtime_device_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to DTensor\n",
    "\n",
    "The following sections are intended to mirror the PyTorch natice DTensor\n",
    "tutorial:\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLAShardedTensor(tensor([[ 0.1726, -0.0877, -0.4218,  ..., -2.0128,  3.0877,  2.9216],\n",
      "        [-1.4800,  1.0192,  0.0603,  ...,  1.2051, -0.0683, -0.4875],\n",
      "        [-0.0424,  0.4998, -0.8199,  ..., -1.4720,  1.2683,  0.7452],\n",
      "        ...,\n",
      "        [ 0.7318, -0.5922,  1.1534,  ..., -2.0714, -0.5528, -0.9685],\n",
      "        [-0.8351, -1.0222, -0.3555,  ...,  0.4233,  1.4786, -0.8307],\n",
      "        [-0.1739,  0.3670,  0.2707,  ...,  1.0409, -0.1011, -0.3389]],\n",
      "       device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#introduction\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.distributed.tensor import init_device_mesh, Shard, distribute_tensor\n",
    "\n",
    "# Create a mesh topology with the available devices:\n",
    "# 1. We can directly create the mesh using elastic launcher, (recommended)\n",
    "# 2. If using mp.spawn, one need to initialize the world process_group first and set device\n",
    "#   i.e. torch.distributed.init_process_group(backend=\"nccl\", world_size=world_size)\n",
    "torch_xla.runtime.use_spmd()\n",
    "mesh = init_device_mesh(\"xla\", (int(os.environ[\"WORLD_SIZE\"]),))\n",
    "big_tensor = torch.randn(100000, 88)\n",
    "# Shard this tensor over the mesh by sharding `big_tensor`'s 0th dimension over the 0th dimension of `mesh`.\n",
    "my_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)])\n",
    "\n",
    "print(my_dtensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DTensor Examples\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#basic-dtensor-api-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLAShardedTensor(tensor([[ 1.9342, -1.0381, -0.5070,  ..., -0.3721, -0.4129, -0.2746],\n",
      "        [ 0.7169,  0.0844, -1.3985,  ..., -0.4495,  1.0836,  0.8900],\n",
      "        [-1.7920, -0.4673,  1.1471,  ...,  0.4380, -0.2053,  0.2884],\n",
      "        ...,\n",
      "        [ 1.0982,  0.2193, -0.5645,  ..., -0.1739,  0.0825, -1.8456],\n",
      "        [-1.3523,  0.4113, -0.7503,  ...,  0.3793,  0.3068,  1.2890],\n",
      "        [-0.2711, -0.3994,  0.8650,  ..., -0.4849,  0.3394, -0.3395]],\n",
      "       device='xla:0'))\n",
      "XLAShardedTensor(tensor([[ 1.9342, -1.0381, -0.5070,  ..., -0.3721, -0.4129, -0.2746],\n",
      "        [ 0.7169,  0.0844, -1.3985,  ..., -0.4495,  1.0836,  0.8900],\n",
      "        [-1.7920, -0.4673,  1.1471,  ...,  0.4380, -0.2053,  0.2884],\n",
      "        ...,\n",
      "        [ 1.0982,  0.2193, -0.5645,  ..., -0.1739,  0.0825, -1.8456],\n",
      "        [-1.3523,  0.4113, -0.7503,  ...,  0.3793,  0.3068,  1.2890],\n",
      "        [-0.2711, -0.3994,  0.8650,  ..., -0.4849,  0.3394, -0.3395]],\n",
      "       device='xla:0'))\n",
      "XLAShardedTensor(tensor([[ 1.9342, -1.0381, -0.5070,  ..., -0.3721, -0.4129, -0.2746],\n",
      "        [ 0.7169,  0.0844, -1.3985,  ..., -0.4495,  1.0836,  0.8900],\n",
      "        [-1.7920, -0.4673,  1.1471,  ...,  0.4380, -0.2053,  0.2884],\n",
      "        ...,\n",
      "        [ 1.0982,  0.2193, -0.5645,  ..., -0.1739,  0.0825, -1.8456],\n",
      "        [-1.3523,  0.4113, -0.7503,  ...,  0.3793,  0.3068,  1.2890],\n",
      "        [-0.2711, -0.3994,  0.8650,  ..., -0.4849,  0.3394, -0.3395]],\n",
      "       device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributed.tensor import DTensor, Shard, Replicate, distribute_tensor, distribute_module, init_device_mesh\n",
    "\n",
    "# construct a device mesh with available devices (multi-host or single host)\n",
    "device_mesh = init_device_mesh(\"xla\", (4,))\n",
    "# if we want to do row-wise sharding\n",
    "rowwise_placement=[Shard(0)]\n",
    "# if we want to do col-wise sharding\n",
    "colwise_placement=[Shard(1)]\n",
    "\n",
    "big_tensor = torch.randn(888, 12)\n",
    "# distributed tensor returned will be sharded across the dimension specified in placements\n",
    "rowwise_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=rowwise_placement)\n",
    "\n",
    "# if we want to do replication across a certain device list\n",
    "replica_placement = [Replicate()]\n",
    "# distributed tensor will be replicated to all four GPUs.\n",
    "replica_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=replica_placement)\n",
    "\n",
    "# if we want to distributed a tensor with both replication and sharding\n",
    "device_mesh = init_device_mesh(\"xla\", (2, 2))\n",
    "# replicate across the first dimension of device mesh, then sharding on the second dimension of device mesh\n",
    "spec=[Replicate(), Shard(0)]\n",
    "partial_replica = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=spec)\n",
    "\n",
    "\n",
    "print(rowwise_tensor)\n",
    "print(replica_tensor)\n",
    "print(partial_replica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UX ISSUE: `DTensor.from_local` fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeviceMesh' object has no attribute '_coordinate_on_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\u001b[39;00m\n\u001b[32m      2\u001b[39m local_tensor = torch.randn((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m rowwise_tensor = \u001b[43mDTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowwise_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# reshard the current row-wise tensor to a colwise tensor or replicate tensor\u001b[39;00m\n\u001b[32m      6\u001b[39m colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:432\u001b[39m, in \u001b[36mDTensor.from_local\u001b[39m\u001b[34m(local_tensor, device_mesh, placements, run_check, shape, stride)\u001b[39m\n\u001b[32m    427\u001b[39m                 placements[idx] = Shard(placement.dim + local_tensor.ndim)\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# `from_local` is differentiable, and the gradient of the dist tensor this function\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# created should flow back the gradients to the local_tensor, so we call an autograd\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# function to construct the dist tensor instead.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FromTorchTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pyre-ignore[16]: autograd func\u001b[39;49;00m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplacements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:148\u001b[39m, in \u001b[36m_FromTorchTensor.forward\u001b[39m\u001b[34m(ctx, input, device_mesh, placements, run_check, shape, stride)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    144\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, stride:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstride\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass both shape and stride at the same time.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdevice_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# if the global rank is not participating in the device mesh, we\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# simply set the local tensor to an empty tensor\u001b[39;00m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.new_empty(\u001b[32m0\u001b[39m, requires_grad=\u001b[38;5;28minput\u001b[39m.requires_grad)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m run_check:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# TODO: support uneven sharding when global shape/stride not passed, by\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# building the global TensorMeta during check_tensor_meta\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/device_mesh.py:957\u001b[39m, in \u001b[36mDeviceMesh.get_coordinate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_coordinate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    953\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[33;03m    Return the relative indices of this rank relative to all\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[33;03m    dimensions of the mesh. If this rank is not part of the mesh, return None.\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coordinate_on_dim \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_coordinate_on_dim\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'DeviceMesh' object has no attribute '_coordinate_on_dim'"
     ]
    }
   ],
   "source": [
    "# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
    "local_tensor = torch.randn((8, 8), requires_grad=True)\n",
    "rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n",
    "\n",
    "# reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
    "colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
    "replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted workaround\n",
    "\n",
    "Sort of... New error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device_mesh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# See: https://github.com/pytorch/xla/issues/8528\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Need to stub the method in the meantime.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Not sure what to stub it to though.\u001b[39;00m\n\u001b[32m      4\u001b[39m _rank = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdevice_mesh\u001b[49m._coordinate_on_dim = [_rank]  \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\u001b[39;00m\n\u001b[32m      8\u001b[39m local_tensor = torch.randn((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'device_mesh' is not defined"
     ]
    }
   ],
   "source": [
    "# See: https://github.com/pytorch/xla/issues/8528\n",
    "# Need to stub the method in the meantime.\n",
    "# Not sure what to stub it to though.\n",
    "_rank = 0\n",
    "device_mesh._coordinate_on_dim = [_rank]  \n",
    "\n",
    "# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
    "local_tensor = torch.randn((8, 8), requires_grad=True)\n",
    "rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n",
    "\n",
    "# reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
    "colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
    "replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DummyMLP Example from Torch Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyMLP(\n",
       "  (net1): Linear(in_features=5, out_features=1024, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (net2): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DummyMLP(torch.nn.Module):\n",
    "  def __init__(self, device):\n",
    "    super().__init__()\n",
    "    self.net1 = torch.nn.Linear(5, 1024, device=device)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.net2 = torch.nn.Linear(1024, 4, device=device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net2(F.relu(self.net1(x)))\n",
    "\n",
    "  def reset_parameters(self, *args, **kwargs):\n",
    "    with torch.no_grad():\n",
    "      self.net1.weight.fill_(0.5)\n",
    "      self.net2.weight.fill_(1)\n",
    "      self.net1.bias.fill_(1.5)\n",
    "      self.net2.bias.fill_(1.2)\n",
    "\n",
    "DummyMLP(\"xla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UX ISSUE: Can't distribute using `parallelize_module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DummyMLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     ColwiseParallel,\n\u001b[32m      3\u001b[39m     parallelize_module,\n\u001b[32m      4\u001b[39m     RowwiseParallel,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m device_type = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# was \"meta\" in test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mDummyMLP\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m device_mesh = init_device_mesh(device_type, (\u001b[38;5;28mint\u001b[39m(os.environ[\u001b[33m\"\u001b[39m\u001b[33mWORLD_SIZE\u001b[39m\u001b[33m\"\u001b[39m]),))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# UX ISSUE: XLA and PyTorch handle \"RANK\" differently.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# See: https://github.com/pytorch/xla/issues/8528\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Need to stub the method in the meantime.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Not sure what to stub it to though.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'DummyMLP' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.distributed.tensor.parallel import (\n",
    "    ColwiseParallel,\n",
    "    parallelize_module,\n",
    "    RowwiseParallel,\n",
    ")\n",
    "\n",
    "device_type = \"xla\" # was \"meta\" in test\n",
    "\n",
    "model = DummyMLP(\"xla\")\n",
    "device_mesh = init_device_mesh(device_type, (int(os.environ[\"WORLD_SIZE\"]),))\n",
    "\n",
    "# UX ISSUE: XLA and PyTorch handle \"RANK\" differently.\n",
    "# See: https://github.com/pytorch/xla/issues/8528\n",
    "# Need to stub the method in the meantime.\n",
    "# Not sure what to stub it to though.\n",
    "_rank = 0\n",
    "device_mesh._coordinate_on_dim = [_rank]  \n",
    "\n",
    "# UX ISSUE: We can't use upstream parallelization plans\n",
    "parallelize_plan = {\n",
    "    \"net1\": ColwiseParallel(),\n",
    "    \"net2\": RowwiseParallel(),\n",
    "}\n",
    "model_tp = parallelize_module(model, device_mesh, parallelize_plan)\n",
    "model_tp.to_empty(device=device_type)\n",
    "model_tp.reset_parameters()\n",
    "optim = torch.optim.SGD(model_tp.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "inp = torch.randn(20, 5, device=device_type)\n",
    "output = model_tp(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "!at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at \"/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp\":838, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(sharded_module)\n\u001b[32m     29\u001b[39m x = torch.rand((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43msharded_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mMyModule.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.fc2(\u001b[38;5;28minput\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharded_tensor.py:174\u001b[39m, in \u001b[36mXLAShardedTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__torch_function__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, func, types, args=(), kwargs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/_tensor.py:1668\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1668\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1670\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharded_tensor.py:169\u001b[39m, in \u001b[36mXLAShardedTensor.__torch_dispatch__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# no_dispatch is only needed if you use enable_python_mode.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# It prevents infinite recursion.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m no_dispatch():\n\u001b[32m    167\u001b[39m   \u001b[38;5;66;03m# re-dispatch to C++\u001b[39;00m\n\u001b[32m    168\u001b[39m   rs = tree_map(wrap,\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m                 \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/_ops.py:806\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: !at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at \"/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp\":838, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributed.tensor import Shard, distribute_tensor, distribute_module, init_device_mesh\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.relu(self.fc1(input) + self.fc2(input))\n",
    "\n",
    "mesh = init_device_mesh(\"xla\", (4,))\n",
    "\n",
    "def shard_params(mod_name, mod, mesh):\n",
    "    col_linear_placement = [Shard(0)]\n",
    "    # shard fc1 and fc2\n",
    "    if isinstance(mod, nn.Linear):\n",
    "        for name, param in mod.named_parameters():\n",
    "            dist_param = nn.Parameter(\n",
    "                distribute_tensor(param, mesh, col_linear_placement)\n",
    "            )\n",
    "            mod.register_parameter(name, dist_param)\n",
    "\n",
    "sharded_module = distribute_module(MyModule(), mesh, partition_fn=shard_params)\n",
    "print(sharded_module)\n",
    "\n",
    "x = torch.rand((8, 8))\n",
    "sharded_module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLO\n",
      " HloModule IrToHlo.21, entry_computation_layout={(f32[5]{0}, f32[5,32]{1,0}, f32[32]{0}, f32[32,10]{1,0}, f32[20,10]{1,0})->(f32[20,5]{1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.21 (p0.1: f32[5], p1.2: f32[5,32], p2.4: f32[32], p3.5: f32[32,10], p4.7: f32[20,10]) -> (f32[20,5]) {\n",
      "  %p4.7 = f32[20,10]{1,0} parameter(4), sharding={replicated}\n",
      "  %p3.5 = f32[32,10]{1,0} parameter(3), sharding={devices=[4,1]0,1,2,3}\n",
      "  %transpose.6 = f32[10,32]{0,1} transpose(f32[32,10]{1,0} %p3.5), dimensions={1,0}\n",
      "  %dot.8 = f32[20,32]{1,0} dot(f32[20,10]{1,0} %p4.7, f32[10,32]{0,1} %transpose.6), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  %p2.4 = f32[32]{0} parameter(2), sharding={devices=[4]0,1,2,3}\n",
      "  %reshape.9 = f32[1,32]{1,0} reshape(f32[32]{0} %p2.4)\n",
      "  %broadcast.10 = f32[1,32]{1,0} broadcast(f32[1,32]{1,0} %reshape.9), dimensions={0,1}\n",
      "  %reshape.11 = f32[32]{0} reshape(f32[1,32]{1,0} %broadcast.10)\n",
      "  %broadcast.12 = f32[20,32]{1,0} broadcast(f32[32]{0} %reshape.11), dimensions={1}\n",
      "  %add.13 = f32[20,32]{1,0} add(f32[20,32]{1,0} %dot.8, f32[20,32]{1,0} %broadcast.12)\n",
      "  %p1.2 = f32[5,32]{1,0} parameter(1), sharding={devices=[1,4]0,1,2,3}\n",
      "  %transpose.3 = f32[32,5]{0,1} transpose(f32[5,32]{1,0} %p1.2), dimensions={1,0}\n",
      "  %dot.14 = f32[20,5]{1,0} dot(f32[20,32]{1,0} %add.13, f32[32,5]{0,1} %transpose.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  %p0.1 = f32[5]{0} parameter(0), sharding={replicated}\n",
      "  %reshape.15 = f32[1,5]{1,0} reshape(f32[5]{0} %p0.1)\n",
      "  %broadcast.16 = f32[1,5]{1,0} broadcast(f32[1,5]{1,0} %reshape.15), dimensions={0,1}\n",
      "  %reshape.17 = f32[5]{0} reshape(f32[1,5]{1,0} %broadcast.16)\n",
      "  %broadcast.18 = f32[20,5]{1,0} broadcast(f32[5]{0} %reshape.17), dimensions={1}\n",
      "  %add.19 = f32[20,5]{1,0} add(f32[20,5]{1,0} %dot.14, f32[20,5]{1,0} %broadcast.18)\n",
      "  ROOT %tuple.20 = (f32[20,5]{1,0}) tuple(f32[20,5]{1,0} %add.19)\n",
      "}\n",
      "\n",
      "\n",
      "STABLEHLO\n",
      " module @IrToHlo.21 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {\n",
      "  func.func @main(%arg0: tensor<5xf32> {mhlo.sharding = \"{replicated}\"}, %arg1: tensor<5x32xf32> {mhlo.sharding = \"{devices=[1,4]0,1,2,3}\"}, %arg2: tensor<32xf32> {mhlo.sharding = \"{devices=[4]0,1,2,3}\"}, %arg3: tensor<32x10xf32> {mhlo.sharding = \"{devices=[4,1]0,1,2,3}\"}, %arg4: tensor<20x10xf32> {mhlo.sharding = \"{replicated}\"}) -> tensor<20x5xf32> {\n",
      "    %0 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = \"f32[10,32]{0,1}\"} : (tensor<32x10xf32>) -> tensor<10x32xf32>\n",
      "    %1 = stablehlo.dot_general %arg4, %0, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<20x10xf32>, tensor<10x32xf32>) -> tensor<20x32xf32>\n",
      "    %2 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<32xf32>) -> tensor<20x32xf32>\n",
      "    %3 = stablehlo.add %1, %2 : tensor<20x32xf32>\n",
      "    %4 = stablehlo.transpose %arg1, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = \"f32[32,5]{0,1}\"} : (tensor<5x32xf32>) -> tensor<32x5xf32>\n",
      "    %5 = stablehlo.dot_general %3, %4, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<20x32xf32>, tensor<32x5xf32>) -> tensor<20x5xf32>\n",
      "    %6 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<20x5xf32>\n",
      "    %7 = stablehlo.add %5, %6 : tensor<20x5xf32>\n",
      "    return %7 : tensor<20x5xf32>\n",
      "  }\n",
      "}\n",
      "\n",
      "tensor([[-0.0050, -0.1797, -0.1726,  0.0190,  0.3568],\n",
      "        [-0.0731, -0.1416, -0.1998, -0.0990,  0.3375],\n",
      "        [ 0.0730, -0.1952, -0.2346, -0.1345,  0.2603],\n",
      "        [-0.1149, -0.2296, -0.3373, -0.0954,  0.1511],\n",
      "        [ 0.0861, -0.1261, -0.0939,  0.0472,  0.3663],\n",
      "        [ 0.0225, -0.1103, -0.2026, -0.0232,  0.4597],\n",
      "        [-0.0480, -0.2260, -0.1150, -0.0764,  0.1985],\n",
      "        [ 0.0408, -0.1646, -0.1248, -0.1641,  0.3512],\n",
      "        [ 0.0351, -0.1099,  0.0155, -0.1690,  0.2634],\n",
      "        [ 0.0392, -0.0869, -0.1149,  0.0424,  0.2847],\n",
      "        [-0.0819, -0.0467, -0.1583, -0.0247,  0.3695],\n",
      "        [-0.0249, -0.2787, -0.3035, -0.1455,  0.2335],\n",
      "        [ 0.0088, -0.1070, -0.0438, -0.1647,  0.2966],\n",
      "        [-0.0192, -0.1900, -0.1927, -0.0627,  0.3472],\n",
      "        [-0.1426, -0.1845, -0.1738, -0.2358,  0.3349],\n",
      "        [-0.1712, -0.2606, -0.1525, -0.1343,  0.1251],\n",
      "        [-0.0048, -0.2390, -0.1229, -0.0512,  0.2255],\n",
      "        [-0.0503, -0.2687, -0.2371, -0.2891,  0.4015],\n",
      "        [-0.0326, -0.2927, -0.2020,  0.0022,  0.2379],\n",
      "        [-0.1966, -0.1170, -0.1109, -0.1559,  0.2774]],\n",
      "       grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/torch_xla/runtime.py:236: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
      "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "# test_dtensor_toy_model_forward.py\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributed._tensor.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel\n",
    "from torch.distributed.tensor import DTensor, Shard, Replicate, distribute_tensor, distribute_module, init_device_mesh\n",
    "from torch_xla.core import xla_model as xm\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP based model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.in_proj = nn.Linear(10, 32)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.out_proj = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out_proj(self.in_proj(x))\n",
    "        #return self.out_proj(self.relu(self.in_proj(x)))\n",
    "\n",
    "torch_xla.runtime.use_spmd()\n",
    "_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "_rank = int(os.environ.get(\"RANK\", 1))\n",
    "device_name = 'xla' # 'cuda'\n",
    "\n",
    "\n",
    "device_mesh = init_device_mesh(device_type=device_name, mesh_shape=(_world_size,))\n",
    "device_mesh._coordinate_on_dim = [_rank]  # workaround for https://github.com/pytorch/xla/issues/8528\n",
    "torch.manual_seed(1)\n",
    "inp = torch.rand(20, 10).to(device=device_name)\n",
    "tp_model = ToyModel().to(device=device_name)\n",
    "\n",
    "DIST_MODE = 0\n",
    "if DIST_MODE==0:\n",
    "    distribute_tensor(tp_model.in_proj.weight, device_mesh, [Shard(0)])\n",
    "    distribute_tensor(tp_model.in_proj.bias, device_mesh, [Shard(0)])\n",
    "    distribute_tensor(tp_model.out_proj.weight, device_mesh, [Shard(1)])\n",
    "    distribute_tensor(tp_model.out_proj.bias, device_mesh, [Replicate()])\n",
    "elif DIST_MODE == 1:\n",
    "    # This is a hack to just call mark sharding...\n",
    "    # This doesn't replace the weights with XLAShardedTensors\n",
    "    tp_model.in_proj.weight = distribute_tensor(tp_model.in_proj.weight, device_mesh, [Shard(0)])\n",
    "    tp_model.in_proj.bias = distribute_tensor(tp_model.in_proj.bias, device_mesh, [Shard(0)])\n",
    "    tp_model.out_proj.weight = distribute_tensor(tp_model.out_proj.weight, device_mesh, [Shard(1)])\n",
    "    tp_model.out_proj.bias = distribute_tensor(tp_model.out_proj.bias, device_mesh, [Replicate()])\n",
    "    print(type(tp_model.in_proj.weight))\n",
    "elif DIST_MODE == 2:\n",
    "    # This replaces all weights with XLAShardedTensors, however XLAShardedTensors\n",
    "    # dont execute properly today.\n",
    "    tp_model = parallelize_module(\n",
    "            module=tp_model,\n",
    "            device_mesh=device_mesh,\n",
    "            parallelize_plan={\n",
    "                \"in_proj\": ColwiseParallel(),\n",
    "                \"out_proj\": RowwiseParallel(),\n",
    "            },\n",
    "        )\n",
    "    print(type(tp_model.in_proj.weight))\n",
    "    print(tp_model.in_proj.weight.sharding_spec)\n",
    "    print(tp_model.out_proj.weight.sharding_spec)\n",
    "\n",
    "out = tp_model(inp)\n",
    "print(\"HLO\\n\", torch_xla._XLAC._get_xla_tensors_hlo([out]))\n",
    "print(\"STABLEHLO\\n\", xm.get_stablehlo([out]))\n",
    "\n",
    "print(out.cpu())\n",
    "xm.mark_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native PyTorch Tensor Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a sharding plan based on the given world_sizeCreate a sharding plan based on the given world_size 2\n",
      "Create a sharding plan based on the given world_size 3\n",
      "Create a sharding plan based on the given world_size 0\n",
      " 1\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %in_proj : [num_users=1] = call_module[target=in_proj](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_module[target=relu](args = (%in_proj,), kwargs = {})\n",
      "    %out_proj : [num_users=1] = call_module[target=out_proj](args = (%relu,), kwargs = {})\n",
      "    return out_proj\n",
      "Rank0: DTensor(local_tensor=tensor([[ 0.0714,  0.2321, -0.2109,  0.0709, -0.0369, -0.0149, -0.2217,  0.0326,\n",
      "         -0.3099,  0.0043],\n",
      "        [ 0.0765, -0.0198, -0.1454,  0.2559,  0.1785, -0.0122,  0.1996,  0.0504,\n",
      "         -0.1649, -0.1638],\n",
      "        [ 0.2190,  0.0206,  0.0890,  0.2985, -0.2653, -0.1647,  0.1480, -0.2851,\n",
      "          0.2455, -0.1108],\n",
      "        [-0.2619, -0.2695,  0.0860,  0.1910,  0.1654, -0.0758,  0.2099,  0.0857,\n",
      "          0.2804,  0.0879],\n",
      "        [-0.2919,  0.1432, -0.3069, -0.0361, -0.1980,  0.2964,  0.2738,  0.0928,\n",
      "         -0.0796,  0.1802],\n",
      "        [-0.2211, -0.1877,  0.3100, -0.0473, -0.2236,  0.0377, -0.0168, -0.0456,\n",
      "          0.0186, -0.2655],\n",
      "        [ 0.0740,  0.2781,  0.1665, -0.1295,  0.1076, -0.1649, -0.1398,  0.1758,\n",
      "         -0.2736,  0.1121],\n",
      "        [-0.2276,  0.0185, -0.2511,  0.2224,  0.0493, -0.0420, -0.0931,  0.0416,\n",
      "         -0.1527, -0.2148]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "Parallelize the module based on the given Parallel Style\n",
      "Rank3: DTensor(local_tensor=tensor([[ 0.0606, -0.0206, -0.2269, -0.1410,  0.2869,  0.0354, -0.2969,  0.0646,\n",
      "         -0.1483, -0.2020],\n",
      "        [ 0.2145, -0.2494, -0.0081,  0.2529, -0.2947,  0.0485, -0.1979,  0.2163,\n",
      "          0.3142,  0.3136],\n",
      "        [-0.1804,  0.1516, -0.0704, -0.0463, -0.2025, -0.1105, -0.1955,  0.1818,\n",
      "          0.0799,  0.3086],\n",
      "        [-0.0675,  0.2483,  0.2871,  0.0077, -0.1269,  0.0827, -0.1562,  0.2835,\n",
      "         -0.0771,  0.2324],\n",
      "        [-0.0989,  0.2910, -0.3069,  0.1677,  0.0905,  0.0532, -0.0727, -0.0768,\n",
      "          0.2938,  0.2206],\n",
      "        [-0.3073, -0.2676, -0.3069, -0.1024, -0.1994,  0.2964, -0.2756,  0.0514,\n",
      "          0.2263, -0.1750],\n",
      "        [ 0.2590,  0.1706,  0.0987, -0.1509, -0.0412,  0.1341,  0.1972, -0.1894,\n",
      "          0.2206, -0.2567],\n",
      "        [ 0.0346, -0.1095,  0.2640, -0.0226,  0.1850,  0.1454,  0.0862,  0.2553,\n",
      "         -0.2089,  0.0326]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "Rank2: DTensor(local_tensor=tensor([[ 0.0772, -0.1160, -0.0580, -0.1041,  0.0619, -0.1560,  0.1053,  0.1014,\n",
      "          0.3095, -0.2901],\n",
      "        [-0.1446, -0.2173,  0.2073, -0.0457,  0.0159,  0.2117,  0.2943,  0.1076,\n",
      "          0.2794,  0.0179],\n",
      "        [-0.0759,  0.1871, -0.3020,  0.0981,  0.2705, -0.0035,  0.0066, -0.2168,\n",
      "         -0.1635, -0.2704],\n",
      "        [-0.2333, -0.1724,  0.2845, -0.0071,  0.1916,  0.1793, -0.0659,  0.0280,\n",
      "          0.1108, -0.0115],\n",
      "        [-0.0043,  0.1656,  0.2492,  0.3124,  0.2447,  0.2811, -0.2976,  0.0235,\n",
      "         -0.1636, -0.0292],\n",
      "        [ 0.2079, -0.1671, -0.0384,  0.1276,  0.1800,  0.1144,  0.0975, -0.2893,\n",
      "          0.2672, -0.0051],\n",
      "        [ 0.1832, -0.2035, -0.1098,  0.1583,  0.2931, -0.3076, -0.1885,  0.0248,\n",
      "          0.3155,  0.2700],\n",
      "        [ 0.1628, -0.1099,  0.1468,  0.2090,  0.2583,  0.0747,  0.1650, -0.2168,\n",
      "         -0.0179,  0.3056]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "Rank1: DTensor(local_tensor=tensor([[-0.2793, -0.3074, -0.2271, -0.0439, -0.2966,  0.0856, -0.0127,  0.2404,\n",
      "          0.1031,  0.3117],\n",
      "        [-0.0065,  0.2117, -0.1180, -0.0170,  0.2315, -0.0903, -0.2342, -0.0390,\n",
      "         -0.2263,  0.2616],\n",
      "        [ 0.0323, -0.0910,  0.0842,  0.1475, -0.0243,  0.1456, -0.0970, -0.0414,\n",
      "          0.2379,  0.0446],\n",
      "        [ 0.3148,  0.1972, -0.2164,  0.2711,  0.0188, -0.1529, -0.2067, -0.2828,\n",
      "          0.2403,  0.1773],\n",
      "        [ 0.1269,  0.0298, -0.0733,  0.0496, -0.2562,  0.0058,  0.1721,  0.1116,\n",
      "         -0.0440,  0.1821],\n",
      "        [ 0.0502, -0.0795,  0.2967, -0.1683, -0.0196, -0.2151, -0.2169, -0.0437,\n",
      "          0.1405,  0.2408],\n",
      "        [ 0.0351,  0.1919,  0.2306, -0.0655, -0.0514,  0.0711,  0.0229,  0.0831,\n",
      "         -0.2707, -0.1165],\n",
      "        [ 0.0335,  0.1027,  0.2906, -0.0579,  0.0684, -0.0239,  0.3094,  0.0867,\n",
      "          0.1536,  0.2350]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "FWD Step: iter 0\n",
      "BWD Step: iter 0\n",
      "Optimization Step: iter 0\n",
      "FWD Step: iter 1\n",
      "BWD Step: iter 1\n",
      "Optimization Step: iter 1\n",
      "FWD Step: iter 2\n",
      "BWD Step: iter 2\n",
      "Optimization Step: iter 2\n",
      "FWD Step: iter 3\n",
      "BWD Step: iter 3\n",
      "Optimization Step: iter 3\n",
      "FWD Step: iter 4\n",
      "BWD Step: iter 4\n",
      "Optimization Step: iter 4\n",
      "FWD Step: iter 5\n",
      "BWD Step: iter 5\n",
      "Optimization Step: iter 5\n",
      "FWD Step: iter 6\n",
      "BWD Step: iter 6\n",
      "Optimization Step: iter 6\n",
      "FWD Step: iter 7\n",
      "BWD Step: iter 7\n",
      "Optimization Step: iter 7\n",
      "FWD Step: iter 8\n",
      "BWD Step: iter 8\n",
      "Optimization Step: iter 8\n",
      "FWD Step: iter 9\n",
      "BWD Step: iter 9\n",
      "Optimization Step: iter 9\n",
      "FWD Step: iter 10\n",
      "BWD Step: iter 10\n",
      "Optimization Step: iter 10\n",
      "FWD Step: iter 11\n",
      "BWD Step: iter 11\n",
      "Optimization Step: iter 11\n",
      "FWD Step: iter 12\n",
      "BWD Step: iter 12\n",
      "Optimization Step: iter 12\n",
      "FWD Step: iter 13\n",
      "BWD Step: iter 13\n",
      "Optimization Step: iter 13\n",
      "FWD Step: iter 14\n",
      "BWD Step: iter 14\n",
      "Optimization Step: iter 14\n",
      "FWD Step: iter 15\n",
      "BWD Step: iter 15\n",
      "Optimization Step: iter 15\n",
      "FWD Step: iter 16\n",
      "BWD Step: iter 16\n",
      "Optimization Step: iter 16\n",
      "FWD Step: iter 17\n",
      "BWD Step: iter 17\n",
      "Optimization Step: iter 17\n",
      "FWD Step: iter 18\n",
      "BWD Step: iter 18\n",
      "Optimization Step: iter 18\n",
      "FWD Step: iter 19\n",
      "BWD Step: iter 19\n",
      "Optimization Step: iter 19\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms\n",
    "WORLD_SIZE=4\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.distributed._tensor import (\n",
    "    DeviceMesh,\n",
    ")\n",
    "from torch.distributed.tensor.parallel import (\n",
    "    RowwiseParallel,\n",
    "    ColwiseParallel,\n",
    "    parallelize_module,\n",
    ")\n",
    "\n",
    "ITER_TIME = 20\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP based model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.in_proj = nn.Linear(10, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_proj = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out_proj(self.relu(self.in_proj(x)))\n",
    "\n",
    "def print0(msg, rank):\n",
    "    if rank == 0:\n",
    "        print(msg)\n",
    "\n",
    "def printR(msg, rank):\n",
    "    print(f\"Rank{rank}: {msg}\")\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def demo_tp(world_size):\n",
    "    \"\"\"\n",
    "    Main body of the demo of a basic version of tensor parallel by using\n",
    "    PyTorch native APIs.\n",
    "    \"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    print(\"Create a sharding plan based on the given world_size\", rank)\n",
    "    # create a sharding plan based on the given world_size.\n",
    "    device_mesh = DeviceMesh(\n",
    "        \"cpu\",\n",
    "        torch.arange(world_size),\n",
    "    )\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel()\n",
    "    tp_model = parallelize_module(\n",
    "            module=model,\n",
    "            device_mesh=device_mesh,\n",
    "            parallelize_plan={\n",
    "                \"in_proj\": ColwiseParallel(),\n",
    "                \"out_proj\": RowwiseParallel(),\n",
    "            },\n",
    "        )\n",
    "    from torch.fx import symbolic_trace\n",
    "    traced = symbolic_trace(tp_model)\n",
    "    print0(traced.graph, rank)\n",
    "    printR(tp_model.in_proj.weight, rank)\n",
    "\n",
    "    # Create a optimizer for the parallelized module.\n",
    "    LR = 0.25\n",
    "    optimizer = torch.optim.SGD(tp_model.parameters(), lr=LR)\n",
    "    print0(\"Parallelize the module based on the given Parallel Style\", rank)\n",
    "    # Parallelize the module based on the given Parallel Style.\n",
    "\n",
    "    # Perform a num of iterations of forward/backward\n",
    "    # and optimizations for the sharded module.\n",
    "    for i in range(ITER_TIME):\n",
    "        inp = torch.rand(20, 10)\n",
    "        output = tp_model(inp)\n",
    "        print0(f\"FWD Step: iter {i}\", rank)\n",
    "        output.sum().backward()\n",
    "        print0(f\"BWD Step: iter {i}\", rank)\n",
    "        optimizer.step()\n",
    "        print0(f\"Optimization Step: iter {i}\", rank)\n",
    "    print0(\"Training finished\", rank)\n",
    "\n",
    "demo_tp(WORLD_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptxla.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
