{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch/XLA DTensor Integration\n",
    "\n",
    "This notebook focuses on the application of DTensor with PyTorch/XLA.\n",
    "\n",
    "See internal implementation details at\n",
    "[[RFC] XLA Lazy Backend Support In DistributedTensor API #92909][dtensor-rfc]\n",
    "\n",
    "[dtensor-rfc]: https://github.com/pytorch/pytorch/issues/92909\n",
    "\n",
    "This can be run in command line using:\n",
    "\n",
    "```\n",
    "# Install jupyter\n",
    "$ apt install jupyter\n",
    "\n",
    "# Create Jupyter ipy kernel for current development venv:\n",
    "$ pip install ipykernel\n",
    "$ python -m ipykernel install --user --name=ptxla.venv\n",
    "\n",
    "# \n",
    "$ jupyter execute --kernel_name=ptxla.venv /usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/docs/source/perf/dtensor.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parallel environment\n",
    "\n",
    "We'll fake an 8 CPU setup. Note this must be done before the XLA PJRT plugin is\n",
    "initialized by PyTorch/XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'CPU:0'}, {'name': 'CPU:1'}, {'name': 'CPU:2'}, {'name': 'CPU:3'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WORLD_SIZE\"] = '4'\n",
    "os.environ[\"RANK\"] = '1'\n",
    "os.environ[\"CPU_NUM_DEVICES\"] = os.environ[\"WORLD_SIZE\"]\n",
    "os.environ[\"PJRT_DEVICE\"] = 'CPU'\n",
    "\n",
    "import torch_xla\n",
    "torch_xla.runtime.global_runtime_device_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to DTensor\n",
    "\n",
    "The following sections are intended to mirror the PyTorch natice DTensor\n",
    "tutorial:\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLAShardedTensor(tensor([[-2.3854e-01,  5.5333e-01, -6.4550e-01,  ..., -8.1820e-01,\n",
      "          1.3780e+00,  5.1097e-01],\n",
      "        [ 7.3101e-01, -2.2367e+00, -8.9715e-01,  ..., -7.7787e-01,\n",
      "          1.0784e+00,  7.0975e-02],\n",
      "        [-1.3072e+00,  2.0384e+00, -1.9566e-01,  ..., -2.8503e-01,\n",
      "         -2.2741e-01, -4.6136e-01],\n",
      "        ...,\n",
      "        [ 8.4810e-01,  1.3419e-01,  7.7133e-01,  ..., -1.5206e-01,\n",
      "         -6.3849e-01, -2.4455e-03],\n",
      "        [ 4.5250e-01, -3.7841e-01, -2.7445e+00,  ..., -4.5199e-01,\n",
      "          1.4458e+00,  2.7002e-01],\n",
      "        [-4.9910e-01, -1.1933e-01, -1.2691e+00,  ...,  5.2942e-01,\n",
      "         -1.0121e+00, -1.4557e+00]], device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#introduction\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.distributed.tensor import init_device_mesh, Shard, distribute_tensor\n",
    "\n",
    "# Create a mesh topology with the available devices:\n",
    "# 1. We can directly create the mesh using elastic launcher, (recommended)\n",
    "# 2. If using mp.spawn, one need to initialize the world process_group first and set device\n",
    "#   i.e. torch.distributed.init_process_group(backend=\"nccl\", world_size=world_size)\n",
    "torch_xla.runtime.use_spmd()\n",
    "mesh = init_device_mesh(\"xla\", (int(os.environ[\"WORLD_SIZE\"]),))\n",
    "big_tensor = torch.randn(100000, 88)\n",
    "# Shard this tensor over the mesh by sharding `big_tensor`'s 0th dimension over the 0th dimension of `mesh`.\n",
    "my_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)])\n",
    "\n",
    "print(my_dtensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DTensor Examples\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#basic-dtensor-api-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLAShardedTensor(tensor([[ 0.3499,  0.7769,  0.4670,  ...,  0.2456, -0.1884,  1.0635],\n",
      "        [-1.2960, -0.7200,  0.2598,  ..., -0.1129, -0.3152,  0.5862],\n",
      "        [ 2.3719, -0.1890, -0.2029,  ...,  1.2554,  1.9394,  0.1867],\n",
      "        ...,\n",
      "        [ 1.1783, -2.0097, -0.2131,  ..., -0.2364,  0.0429, -0.1690],\n",
      "        [ 0.2073,  0.0102,  1.1132,  ...,  1.1448,  0.2130,  1.2484],\n",
      "        [ 0.6789,  1.9195, -0.5259,  ..., -1.0065, -1.3724,  0.6549]],\n",
      "       device='xla:0'))\n",
      "XLAShardedTensor(tensor([[ 0.3499,  0.7769,  0.4670,  ...,  0.2456, -0.1884,  1.0635],\n",
      "        [-1.2960, -0.7200,  0.2598,  ..., -0.1129, -0.3152,  0.5862],\n",
      "        [ 2.3719, -0.1890, -0.2029,  ...,  1.2554,  1.9394,  0.1867],\n",
      "        ...,\n",
      "        [ 1.1783, -2.0097, -0.2131,  ..., -0.2364,  0.0429, -0.1690],\n",
      "        [ 0.2073,  0.0102,  1.1132,  ...,  1.1448,  0.2130,  1.2484],\n",
      "        [ 0.6789,  1.9195, -0.5259,  ..., -1.0065, -1.3724,  0.6549]],\n",
      "       device='xla:0'))\n",
      "XLAShardedTensor(tensor([[ 0.3499,  0.7769,  0.4670,  ...,  0.2456, -0.1884,  1.0635],\n",
      "        [-1.2960, -0.7200,  0.2598,  ..., -0.1129, -0.3152,  0.5862],\n",
      "        [ 2.3719, -0.1890, -0.2029,  ...,  1.2554,  1.9394,  0.1867],\n",
      "        ...,\n",
      "        [ 1.1783, -2.0097, -0.2131,  ..., -0.2364,  0.0429, -0.1690],\n",
      "        [ 0.2073,  0.0102,  1.1132,  ...,  1.1448,  0.2130,  1.2484],\n",
      "        [ 0.6789,  1.9195, -0.5259,  ..., -1.0065, -1.3724,  0.6549]],\n",
      "       device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributed.tensor import DTensor, Shard, Replicate, distribute_tensor, distribute_module, init_device_mesh\n",
    "\n",
    "# construct a device mesh with available devices (multi-host or single host)\n",
    "device_mesh = init_device_mesh(\"xla\", (4,))\n",
    "# if we want to do row-wise sharding\n",
    "rowwise_placement=[Shard(0)]\n",
    "# if we want to do col-wise sharding\n",
    "colwise_placement=[Shard(1)]\n",
    "\n",
    "big_tensor = torch.randn(888, 12)\n",
    "# distributed tensor returned will be sharded across the dimension specified in placements\n",
    "rowwise_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=rowwise_placement)\n",
    "\n",
    "# if we want to do replication across a certain device list\n",
    "replica_placement = [Replicate()]\n",
    "# distributed tensor will be replicated to all four GPUs.\n",
    "replica_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=replica_placement)\n",
    "\n",
    "# if we want to distributed a tensor with both replication and sharding\n",
    "device_mesh = init_device_mesh(\"xla\", (2, 2))\n",
    "# replicate across the first dimension of device mesh, then sharding on the second dimension of device mesh\n",
    "spec=[Replicate(), Shard(0)]\n",
    "partial_replica = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=spec)\n",
    "\n",
    "\n",
    "print(rowwise_tensor)\n",
    "print(replica_tensor)\n",
    "print(partial_replica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UX ISSUE: `DTensor.from_local` fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeviceMesh' object has no attribute '_coordinate_on_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\u001b[39;00m\n\u001b[32m      2\u001b[39m local_tensor = torch.randn((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m rowwise_tensor = \u001b[43mDTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowwise_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# reshard the current row-wise tensor to a colwise tensor or replicate tensor\u001b[39;00m\n\u001b[32m      6\u001b[39m colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:432\u001b[39m, in \u001b[36mDTensor.from_local\u001b[39m\u001b[34m(local_tensor, device_mesh, placements, run_check, shape, stride)\u001b[39m\n\u001b[32m    427\u001b[39m                 placements[idx] = Shard(placement.dim + local_tensor.ndim)\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# `from_local` is differentiable, and the gradient of the dist tensor this function\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# created should flow back the gradients to the local_tensor, so we call an autograd\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# function to construct the dist tensor instead.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FromTorchTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pyre-ignore[16]: autograd func\u001b[39;49;00m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplacements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:148\u001b[39m, in \u001b[36m_FromTorchTensor.forward\u001b[39m\u001b[34m(ctx, input, device_mesh, placements, run_check, shape, stride)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    144\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, stride:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstride\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass both shape and stride at the same time.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdevice_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# if the global rank is not participating in the device mesh, we\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# simply set the local tensor to an empty tensor\u001b[39;00m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.new_empty(\u001b[32m0\u001b[39m, requires_grad=\u001b[38;5;28minput\u001b[39m.requires_grad)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m run_check:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# TODO: support uneven sharding when global shape/stride not passed, by\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# building the global TensorMeta during check_tensor_meta\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/device_mesh.py:957\u001b[39m, in \u001b[36mDeviceMesh.get_coordinate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_coordinate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    953\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[33;03m    Return the relative indices of this rank relative to all\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[33;03m    dimensions of the mesh. If this rank is not part of the mesh, return None.\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coordinate_on_dim \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_coordinate_on_dim\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'DeviceMesh' object has no attribute '_coordinate_on_dim'"
     ]
    }
   ],
   "source": [
    "# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
    "local_tensor = torch.randn((8, 8), requires_grad=True)\n",
    "rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n",
    "\n",
    "# reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
    "colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
    "replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted workaround\n",
    "\n",
    "Sort of... New error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeviceMesh' object has no attribute '_dim_group_infos'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# reshard the current row-wise tensor to a colwise tensor or replicate tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m colwise_tensor = \u001b[43mrowwise_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mredistribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolwise_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_api.py:556\u001b[39m, in \u001b[36mDTensor.redistribute\u001b[39m\u001b[34m(self, device_mesh, placements, async_op, forward_dtype, backward_dtype)\u001b[39m\n\u001b[32m    553\u001b[39m placements = \u001b[38;5;28mtuple\u001b[39m(placements)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# pyre-fixme[16]: `Redistribute` has no attribute `apply`.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRedistribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masync_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_dtype\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_redistribute.py:325\u001b[39m, in \u001b[36mRedistribute.forward\u001b[39m\u001b[34m(ctx, input, device_mesh, placements, async_op, forward_dtype, backward_dtype)\u001b[39m\n\u001b[32m    320\u001b[39m     target_spec = DTensorSpec(\n\u001b[32m    321\u001b[39m         device_mesh, placements, tensor_meta=current_spec.tensor_meta\n\u001b[32m    322\u001b[39m     )\n\u001b[32m    324\u001b[39m     local_tensor = \u001b[38;5;28minput\u001b[39m._local_tensor\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     output = \u001b[43mredistribute_local_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masync_op\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_op\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# use the same local tensor if placements are the same.\u001b[39;00m\n\u001b[32m    330\u001b[39m     output = local_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_redistribute.py:239\u001b[39m, in \u001b[36mredistribute_local_tensor\u001b[39m\u001b[34m(local_tensor, current_spec, target_spec, async_op, is_backward)\u001b[39m\n\u001b[32m    237\u001b[39m         shard_spec = cast(Shard, current)\n\u001b[32m    238\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m shard_spec.dim != target_placement.dim:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m             new_local_tensor = \u001b[43mshard_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_new_shard_dim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtransform_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtarget_placement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m target.is_partial():\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current.is_replicate():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/placement_types.py:323\u001b[39m, in \u001b[36mShard._to_new_shard_dim\u001b[39m\u001b[34m(self, local_tensor, mesh, mesh_dim, current_logical_shape, new_shard_dim)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_tensor.is_contiguous():\n\u001b[32m    321\u001b[39m     local_tensor = local_tensor.contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m new_tensor = \u001b[43mshard_dim_alltoall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_shard_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_dim\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m old_dim_padding:\n\u001b[32m    328\u001b[39m     old_dim_unpad_size = (\n\u001b[32m    329\u001b[39m         old_dim_full_chunk_size * num_chunks - current_logical_shape[\u001b[38;5;28mself\u001b[39m.dim]  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[32m    330\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/tensor/_collective_utils.py:67\u001b[39m, in \u001b[36mshard_dim_alltoall\u001b[39m\u001b[34m(input, gather_dim, shard_dim, mesh, mesh_dim)\u001b[39m\n\u001b[32m     62\u001b[39m     out = torch.chunk(out, mesh.size(mesh_dim), dim=shard_dim)[\n\u001b[32m     63\u001b[39m         mesh.get_local_rank(mesh_dim)\n\u001b[32m     64\u001b[39m     ]\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out.contiguous()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m group_name = \u001b[43mfuncol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_resolve_group_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# TODO: enable async op for shard_dim_alltoall\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.ops._dtensor.shard_dim_alltoall(\n\u001b[32m     70\u001b[39m     \u001b[38;5;28minput\u001b[39m, gather_dim, shard_dim, group_name\n\u001b[32m     71\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/_functional_collectives.py:779\u001b[39m, in \u001b[36m_resolve_group_name\u001b[39m\u001b[34m(group, tag)\u001b[39m\n\u001b[32m    777\u001b[39m     dmesh = group[\u001b[32m0\u001b[39m]\n\u001b[32m    778\u001b[39m     dim = group[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdmesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dim_group_infos\u001b[49m[dim][\u001b[32m2\u001b[39m]\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid tuple for group must be (DeviceMesh, int)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DeviceMesh' object has no attribute '_dim_group_infos'"
     ]
    }
   ],
   "source": [
    "# See: https://github.com/pytorch/xla/issues/8528\n",
    "# Need to stub the method in the meantime.\n",
    "# Not sure what to stub it to though.\n",
    "_rank = 0\n",
    "device_mesh._coordinate_on_dim = [_rank]  \n",
    "\n",
    "# create a DistributedTensor that shards on dim 0, from a local torch.Tensor\n",
    "local_tensor = torch.randn((8, 8), requires_grad=True)\n",
    "rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n",
    "\n",
    "# reshard the current row-wise tensor to a colwise tensor or replicate tensor\n",
    "colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\n",
    "replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DummyMLP Example from Torch Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyMLP(\n",
       "  (net1): Linear(in_features=5, out_features=1024, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (net2): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DummyMLP(torch.nn.Module):\n",
    "  def __init__(self, device):\n",
    "    super().__init__()\n",
    "    self.net1 = torch.nn.Linear(5, 1024, device=device)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.net2 = torch.nn.Linear(1024, 4, device=device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net2(F.relu(self.net1(x)))\n",
    "\n",
    "  def reset_parameters(self, *args, **kwargs):\n",
    "    with torch.no_grad():\n",
    "      self.net1.weight.fill_(0.5)\n",
    "      self.net2.weight.fill_(1)\n",
    "      self.net1.bias.fill_(1.5)\n",
    "      self.net2.bias.fill_(1.2)\n",
    "\n",
    "DummyMLP(\"xla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UX ISSUE: Can't distribute using `parallelize_module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m model = DummyMLP(\u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m device_mesh = init_device_mesh(device_type, (\u001b[38;5;28mint\u001b[39m(os.environ[\u001b[33m\"\u001b[39m\u001b[33mWORLD_SIZE\u001b[39m\u001b[33m\"\u001b[39m]),))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m( \u001b[43mdevice_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m _rank = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m device_mesh._coordinate_on_dim = [_rank]  \u001b[38;5;66;03m# workaround for https://github.com/pytorch/xla/issues/8528\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/device_mesh.py:908\u001b[39m, in \u001b[36mDeviceMesh.get_rank\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_rank\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    905\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[33;03m    Returns the current global rank.\u001b[39;00m\n\u001b[32m    907\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:2301\u001b[39m, in \u001b[36mget_rank\u001b[39m\u001b[34m(group)\u001b[39m\n\u001b[32m   2298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[32m   2299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m default_pg = \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember.WORLD:\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg.rank()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1302\u001b[39m, in \u001b[36m_get_default_group\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefault process group has not been initialized, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease make sure to call init_process_group.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1305\u001b[39m     )\n\u001b[32m   1306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember.WORLD)\n",
      "\u001b[31mValueError\u001b[39m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "from torch.distributed.tensor.parallel import (\n",
    "    ColwiseParallel,\n",
    "    parallelize_module,\n",
    "    RowwiseParallel,\n",
    ")\n",
    "\n",
    "device_type = \"xla\" # was \"meta\" in test\n",
    "\n",
    "model = DummyMLP(\"xla\")\n",
    "device_mesh = init_device_mesh(device_type, (int(os.environ[\"WORLD_SIZE\"]),))\n",
    "\n",
    "# UX ISSUE: XLA and PyTorch handle \"RANK\" differently.\n",
    "# See: https://github.com/pytorch/xla/issues/8528\n",
    "# Need to stub the method in the meantime.\n",
    "# Not sure what to stub it to though.\n",
    "_rank = 0\n",
    "device_mesh._coordinate_on_dim = [_rank]  \n",
    "\n",
    "# UX ISSUE: We can't use upstream parallelization plans\n",
    "parallelize_plan = {\n",
    "    \"net1\": ColwiseParallel(),\n",
    "    \"net2\": RowwiseParallel(),\n",
    "}\n",
    "model_tp = parallelize_module(model, device_mesh, parallelize_plan)\n",
    "model_tp.to_empty(device=device_type)\n",
    "model_tp.reset_parameters()\n",
    "optim = torch.optim.SGD(model_tp.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "inp = torch.randn(20, 5, device=device_type)\n",
    "output = model_tp(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/xla/torch_xla/runtime.py:236: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
      "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "!at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at \"/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp\":838, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(sharded_module)\n\u001b[32m     29\u001b[39m x = torch.rand((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43msharded_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mMyModule.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.fc2(\u001b[38;5;28minput\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharded_tensor.py:174\u001b[39m, in \u001b[36mXLAShardedTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__torch_function__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, func, types, args=(), kwargs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/_tensor.py:1668\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1668\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1670\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/pytorch/xla/torch_xla/distributed/spmd/xla_sharded_tensor.py:169\u001b[39m, in \u001b[36mXLAShardedTensor.__torch_dispatch__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# no_dispatch is only needed if you use enable_python_mode.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# It prevents infinite recursion.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m no_dispatch():\n\u001b[32m    167\u001b[39m   \u001b[38;5;66;03m# re-dispatch to C++\u001b[39;00m\n\u001b[32m    168\u001b[39m   rs = tree_map(wrap,\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m                 \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/pytorch/ptxla.venv/lib/python3.11/site-packages/torch/_ops.py:806\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: !at::functionalization::impl::isFunctionalTensor(t) INTERNAL ASSERT FAILED at \"/usr/local/google/home/gleasonk/Coding/pytorch/pytorch/aten/src/ATen/FunctionalTensorWrapper.cpp\":838, please report a bug to PyTorch. The composite op functionalization fallback expects its inputs all not to be functional tensors"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributed.tensor import Shard, distribute_tensor, distribute_module, init_device_mesh\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.relu(self.fc1(input) + self.fc2(input))\n",
    "\n",
    "mesh = init_device_mesh(\"xla\", (4,))\n",
    "\n",
    "def shard_params(mod_name, mod, mesh):\n",
    "    col_linear_placement = [Shard(0)]\n",
    "    # shard fc1 and fc2\n",
    "    if isinstance(mod, nn.Linear):\n",
    "        for name, param in mod.named_parameters():\n",
    "            dist_param = nn.Parameter(\n",
    "                distribute_tensor(param, mesh, col_linear_placement)\n",
    "            )\n",
    "            mod.register_parameter(name, dist_param)\n",
    "\n",
    "sharded_module = distribute_module(MyModule(), mesh, partition_fn=shard_params)\n",
    "print(sharded_module)\n",
    "\n",
    "x = torch.rand((8, 8))\n",
    "sharded_module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch_xla' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out_proj(\u001b[38;5;28mself\u001b[39m.in_proj(x))\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m#return self.out_proj(self.relu(self.in_proj(x)))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtorch_xla\u001b[49m.runtime.use_spmd()\n\u001b[32m     24\u001b[39m _world_size = \u001b[38;5;28mint\u001b[39m(os.environ[\u001b[33m\"\u001b[39m\u001b[33mWORLD_SIZE\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     25\u001b[39m _rank = \u001b[38;5;28mint\u001b[39m(os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mRANK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'torch_xla' is not defined"
     ]
    }
   ],
   "source": [
    "# test_dtensor_toy_model_forward.py\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributed._tensor.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel\n",
    "from torch_xla.core import xla_model as xm\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP based model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.in_proj = nn.Linear(10, 32)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.out_proj = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out_proj(self.in_proj(x))\n",
    "        #return self.out_proj(self.relu(self.in_proj(x)))\n",
    "\n",
    "torch_xla.runtime.use_spmd()\n",
    "_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "_rank = int(os.environ.get(\"RANK\", 1))\n",
    "device_name = 'xla' # 'cuda'\n",
    "\n",
    "\n",
    "device_mesh = init_device_mesh(device_type=device_name, mesh_shape=(_world_size,))\n",
    "device_mesh._coordinate_on_dim = [_rank]  # workaround for https://github.com/pytorch/xla/issues/8528\n",
    "torch.manual_seed(1)\n",
    "inp = torch.rand(20, 10).to(device=device_name)\n",
    "tp_model = ToyModel().to(device=device_name)\n",
    "\n",
    "DIST_MODE = 2\n",
    "if DIST_MODE==0:\n",
    "    distribute_tensor(tp_model.in_proj.weight, device_mesh, [Shard(0)])\n",
    "    distribute_tensor(tp_model.in_proj.bias, device_mesh, [Shard(0)])\n",
    "    distribute_tensor(tp_model.out_proj.weight, device_mesh, [Shard(1)])\n",
    "    distribute_tensor(tp_model.out_proj.bias, device_mesh, [Replicate()])\n",
    "elif DIST_MODE == 1:\n",
    "    # This is a hack to just call mark sharding...\n",
    "    # This doesn't replace the weights with XLAShardedTensors\n",
    "    tp_model.in_proj.weight = distribute_tensor(tp_model.in_proj.weight, device_mesh, [Shard(0)])\n",
    "    tp_model.in_proj.bias = distribute_tensor(tp_model.in_proj.bias, device_mesh, [Shard(0)])\n",
    "    tp_model.out_proj.weight = distribute_tensor(tp_model.out_proj.weight, device_mesh, [Shard(1)])\n",
    "    tp_model.out_proj.bias = distribute_tensor(tp_model.out_proj.bias, device_mesh, [Replicate()])\n",
    "    print(type(tp_model.in_proj.weight))\n",
    "elif DIST_MODE == 2:\n",
    "    # This replaces all weights with XLAShardedTensors, however XLAShardedTensors\n",
    "    # dont execute properly today.\n",
    "    tp_model = parallelize_module(\n",
    "            module=tp_model,\n",
    "            device_mesh=device_mesh,\n",
    "            parallelize_plan={\n",
    "                \"in_proj\": ColwiseParallel(),\n",
    "                \"out_proj\": RowwiseParallel(),\n",
    "            },\n",
    "        )\n",
    "    print(type(tp_model.in_proj.weight))\n",
    "    print(tp_model.in_proj.weight.sharding_spec)\n",
    "    print(tp_model.out_proj.weight.sharding_spec)\n",
    "\n",
    "out = tp_model(inp)\n",
    "print(\"STABLEHLO\\n\", xm.get_stablehlo([out]))\n",
    "\n",
    "print(out.cpu())\n",
    "xm.mark_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native PyTorch Tensor Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a sharding plan based on the given world_sizeCreate a sharding plan based on the given world_size 2\n",
      "Create a sharding plan based on the given world_size 3\n",
      "Create a sharding plan based on the given world_size 0\n",
      " 1\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %in_proj : [num_users=1] = call_module[target=in_proj](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_module[target=relu](args = (%in_proj,), kwargs = {})\n",
      "    %out_proj : [num_users=1] = call_module[target=out_proj](args = (%relu,), kwargs = {})\n",
      "    return out_proj\n",
      "Rank0: DTensor(local_tensor=tensor([[ 0.0714,  0.2321, -0.2109,  0.0709, -0.0369, -0.0149, -0.2217,  0.0326,\n",
      "         -0.3099,  0.0043],\n",
      "        [ 0.0765, -0.0198, -0.1454,  0.2559,  0.1785, -0.0122,  0.1996,  0.0504,\n",
      "         -0.1649, -0.1638],\n",
      "        [ 0.2190,  0.0206,  0.0890,  0.2985, -0.2653, -0.1647,  0.1480, -0.2851,\n",
      "          0.2455, -0.1108],\n",
      "        [-0.2619, -0.2695,  0.0860,  0.1910,  0.1654, -0.0758,  0.2099,  0.0857,\n",
      "          0.2804,  0.0879],\n",
      "        [-0.2919,  0.1432, -0.3069, -0.0361, -0.1980,  0.2964,  0.2738,  0.0928,\n",
      "         -0.0796,  0.1802],\n",
      "        [-0.2211, -0.1877,  0.3100, -0.0473, -0.2236,  0.0377, -0.0168, -0.0456,\n",
      "          0.0186, -0.2655],\n",
      "        [ 0.0740,  0.2781,  0.1665, -0.1295,  0.1076, -0.1649, -0.1398,  0.1758,\n",
      "         -0.2736,  0.1121],\n",
      "        [-0.2276,  0.0185, -0.2511,  0.2224,  0.0493, -0.0420, -0.0931,  0.0416,\n",
      "         -0.1527, -0.2148]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "Parallelize the module based on the given Parallel Style\n",
      "Rank3: DTensor(local_tensor=tensor([[ 0.0606, -0.0206, -0.2269, -0.1410,  0.2869,  0.0354, -0.2969,  0.0646,\n",
      "         -0.1483, -0.2020],\n",
      "        [ 0.2145, -0.2494, -0.0081,  0.2529, -0.2947,  0.0485, -0.1979,  0.2163,\n",
      "          0.3142,  0.3136],\n",
      "        [-0.1804,  0.1516, -0.0704, -0.0463, -0.2025, -0.1105, -0.1955,  0.1818,\n",
      "          0.0799,  0.3086],\n",
      "        [-0.0675,  0.2483,  0.2871,  0.0077, -0.1269,  0.0827, -0.1562,  0.2835,\n",
      "         -0.0771,  0.2324],\n",
      "        [-0.0989,  0.2910, -0.3069,  0.1677,  0.0905,  0.0532, -0.0727, -0.0768,\n",
      "          0.2938,  0.2206],\n",
      "        [-0.3073, -0.2676, -0.3069, -0.1024, -0.1994,  0.2964, -0.2756,  0.0514,\n",
      "          0.2263, -0.1750],\n",
      "        [ 0.2590,  0.1706,  0.0987, -0.1509, -0.0412,  0.1341,  0.1972, -0.1894,\n",
      "          0.2206, -0.2567],\n",
      "        [ 0.0346, -0.1095,  0.2640, -0.0226,  0.1850,  0.1454,  0.0862,  0.2553,\n",
      "         -0.2089,  0.0326]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "Rank2: DTensor(local_tensor=tensor([[ 0.0772, -0.1160, -0.0580, -0.1041,  0.0619, -0.1560,  0.1053,  0.1014,\n",
      "          0.3095, -0.2901],\n",
      "        [-0.1446, -0.2173,  0.2073, -0.0457,  0.0159,  0.2117,  0.2943,  0.1076,\n",
      "          0.2794,  0.0179],\n",
      "        [-0.0759,  0.1871, -0.3020,  0.0981,  0.2705, -0.0035,  0.0066, -0.2168,\n",
      "         -0.1635, -0.2704],\n",
      "        [-0.2333, -0.1724,  0.2845, -0.0071,  0.1916,  0.1793, -0.0659,  0.0280,\n",
      "          0.1108, -0.0115],\n",
      "        [-0.0043,  0.1656,  0.2492,  0.3124,  0.2447,  0.2811, -0.2976,  0.0235,\n",
      "         -0.1636, -0.0292],\n",
      "        [ 0.2079, -0.1671, -0.0384,  0.1276,  0.1800,  0.1144,  0.0975, -0.2893,\n",
      "          0.2672, -0.0051],\n",
      "        [ 0.1832, -0.2035, -0.1098,  0.1583,  0.2931, -0.3076, -0.1885,  0.0248,\n",
      "          0.3155,  0.2700],\n",
      "        [ 0.1628, -0.1099,  0.1468,  0.2090,  0.2583,  0.0747,  0.1650, -0.2168,\n",
      "         -0.0179,  0.3056]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "Rank1: DTensor(local_tensor=tensor([[-0.2793, -0.3074, -0.2271, -0.0439, -0.2966,  0.0856, -0.0127,  0.2404,\n",
      "          0.1031,  0.3117],\n",
      "        [-0.0065,  0.2117, -0.1180, -0.0170,  0.2315, -0.0903, -0.2342, -0.0390,\n",
      "         -0.2263,  0.2616],\n",
      "        [ 0.0323, -0.0910,  0.0842,  0.1475, -0.0243,  0.1456, -0.0970, -0.0414,\n",
      "          0.2379,  0.0446],\n",
      "        [ 0.3148,  0.1972, -0.2164,  0.2711,  0.0188, -0.1529, -0.2067, -0.2828,\n",
      "          0.2403,  0.1773],\n",
      "        [ 0.1269,  0.0298, -0.0733,  0.0496, -0.2562,  0.0058,  0.1721,  0.1116,\n",
      "         -0.0440,  0.1821],\n",
      "        [ 0.0502, -0.0795,  0.2967, -0.1683, -0.0196, -0.2151, -0.2169, -0.0437,\n",
      "          0.1405,  0.2408],\n",
      "        [ 0.0351,  0.1919,  0.2306, -0.0655, -0.0514,  0.0711,  0.0229,  0.0831,\n",
      "         -0.2707, -0.1165],\n",
      "        [ 0.0335,  0.1027,  0.2906, -0.0579,  0.0684, -0.0239,  0.3094,  0.0867,\n",
      "          0.1536,  0.2350]]), device_mesh=DeviceMesh('cpu', [0, 1, 2, 3]), placements=(Shard(dim=0),))\n",
      "FWD Step: iter 0\n",
      "BWD Step: iter 0\n",
      "Optimization Step: iter 0\n",
      "FWD Step: iter 1\n",
      "BWD Step: iter 1\n",
      "Optimization Step: iter 1\n",
      "FWD Step: iter 2\n",
      "BWD Step: iter 2\n",
      "Optimization Step: iter 2\n",
      "FWD Step: iter 3\n",
      "BWD Step: iter 3\n",
      "Optimization Step: iter 3\n",
      "FWD Step: iter 4\n",
      "BWD Step: iter 4\n",
      "Optimization Step: iter 4\n",
      "FWD Step: iter 5\n",
      "BWD Step: iter 5\n",
      "Optimization Step: iter 5\n",
      "FWD Step: iter 6\n",
      "BWD Step: iter 6\n",
      "Optimization Step: iter 6\n",
      "FWD Step: iter 7\n",
      "BWD Step: iter 7\n",
      "Optimization Step: iter 7\n",
      "FWD Step: iter 8\n",
      "BWD Step: iter 8\n",
      "Optimization Step: iter 8\n",
      "FWD Step: iter 9\n",
      "BWD Step: iter 9\n",
      "Optimization Step: iter 9\n",
      "FWD Step: iter 10\n",
      "BWD Step: iter 10\n",
      "Optimization Step: iter 10\n",
      "FWD Step: iter 11\n",
      "BWD Step: iter 11\n",
      "Optimization Step: iter 11\n",
      "FWD Step: iter 12\n",
      "BWD Step: iter 12\n",
      "Optimization Step: iter 12\n",
      "FWD Step: iter 13\n",
      "BWD Step: iter 13\n",
      "Optimization Step: iter 13\n",
      "FWD Step: iter 14\n",
      "BWD Step: iter 14\n",
      "Optimization Step: iter 14\n",
      "FWD Step: iter 15\n",
      "BWD Step: iter 15\n",
      "Optimization Step: iter 15\n",
      "FWD Step: iter 16\n",
      "BWD Step: iter 16\n",
      "Optimization Step: iter 16\n",
      "FWD Step: iter 17\n",
      "BWD Step: iter 17\n",
      "Optimization Step: iter 17\n",
      "FWD Step: iter 18\n",
      "BWD Step: iter 18\n",
      "Optimization Step: iter 18\n",
      "FWD Step: iter 19\n",
      "BWD Step: iter 19\n",
      "Optimization Step: iter 19\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms\n",
    "WORLD_SIZE=4\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.distributed._tensor import (\n",
    "    DeviceMesh,\n",
    ")\n",
    "from torch.distributed.tensor.parallel import (\n",
    "    RowwiseParallel,\n",
    "    ColwiseParallel,\n",
    "    parallelize_module,\n",
    ")\n",
    "\n",
    "ITER_TIME = 20\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP based model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.in_proj = nn.Linear(10, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_proj = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out_proj(self.relu(self.in_proj(x)))\n",
    "\n",
    "def print0(msg, rank):\n",
    "    if rank == 0:\n",
    "        print(msg)\n",
    "\n",
    "def printR(msg, rank):\n",
    "    print(f\"Rank{rank}: {msg}\")\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def demo_tp(world_size):\n",
    "    \"\"\"\n",
    "    Main body of the demo of a basic version of tensor parallel by using\n",
    "    PyTorch native APIs.\n",
    "    \"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    print(\"Create a sharding plan based on the given world_size\", rank)\n",
    "    # create a sharding plan based on the given world_size.\n",
    "    device_mesh = DeviceMesh(\n",
    "        \"cpu\",\n",
    "        torch.arange(world_size),\n",
    "    )\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel()\n",
    "    tp_model = parallelize_module(\n",
    "            module=model,\n",
    "            device_mesh=device_mesh,\n",
    "            parallelize_plan={\n",
    "                \"in_proj\": ColwiseParallel(),\n",
    "                \"out_proj\": RowwiseParallel(),\n",
    "            },\n",
    "        )\n",
    "    from torch.fx import symbolic_trace\n",
    "    traced = symbolic_trace(tp_model)\n",
    "    print0(traced.graph, rank)\n",
    "    printR(tp_model.in_proj.weight, rank)\n",
    "\n",
    "    # Create a optimizer for the parallelized module.\n",
    "    LR = 0.25\n",
    "    optimizer = torch.optim.SGD(tp_model.parameters(), lr=LR)\n",
    "    print0(\"Parallelize the module based on the given Parallel Style\", rank)\n",
    "    # Parallelize the module based on the given Parallel Style.\n",
    "\n",
    "    # Perform a num of iterations of forward/backward\n",
    "    # and optimizations for the sharded module.\n",
    "    for i in range(ITER_TIME):\n",
    "        inp = torch.rand(20, 10)\n",
    "        output = tp_model(inp)\n",
    "        print0(f\"FWD Step: iter {i}\", rank)\n",
    "        output.sum().backward()\n",
    "        print0(f\"BWD Step: iter {i}\", rank)\n",
    "        optimizer.step()\n",
    "        print0(f\"Optimization Step: iter {i}\", rank)\n",
    "    print0(\"Training finished\", rank)\n",
    "\n",
    "demo_tp(WORLD_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptxla.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
